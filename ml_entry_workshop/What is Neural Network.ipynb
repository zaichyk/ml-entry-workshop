{
 "cells": [
  {
   "cell_type": "code",
   "id": "749a6e7b82b8045f",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# For interactive visualizations\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# What is a Neural Network? An Intuitive Guide\n",
    "\n",
    "Neural networks are the foundation of modern artificial intelligence and deep learning. This guide will walk you through the concepts, components, and mechanics of neural networks in an intuitive way.\n",
    "\n",
    "## What We'll Cover\n",
    "1. Introduction to Neural Networks\n",
    "2. Biological Inspiration\n",
    "3. Basic Components (Neurons, Layers, Weights)\n",
    "4. Activation Functions\n",
    "5. Forward Propagation\n",
    "6. Simple Neural Network Example\n",
    "7. Training (Backpropagation, Gradient Descent)\n",
    "8. Visualizing a Neural Network\n",
    "9. Applications and Use Cases\n",
    "10. Hands-on Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae764f623ceb818",
   "metadata": {},
   "source": [
    "## 1. Introduction to Neural Networks\n",
    "\n",
    "Neural networks are computational models inspired by the human brain. At their core, they're designed to recognize patterns in data.\n",
    "\n",
    "### What makes neural networks special?\n",
    "\n",
    "1. **Ability to learn from data**: Unlike traditional algorithms that follow explicit instructions, neural networks learn patterns from examples.\n",
    "2. **Adaptability**: They can adjust to new inputs and improve over time.\n",
    "3. **Pattern recognition**: They excel at finding complex patterns that humans might miss.\n",
    "4. **Universal approximation**: Given enough neurons, they can model almost any function.\n",
    "\n",
    "### The analogy of a \"black box\"\n",
    "\n",
    "Think of a neural network as a black box that takes inputs and produces outputs:\n",
    "- **Input**: The raw data (e.g., images, text, numbers)\n",
    "- **Black Box**: The neural network processing\n",
    "- **Output**: Predictions or classifications\n",
    "\n",
    "Let's visualize this simple concept:"
   ]
  },
  {
   "cell_type": "code",
   "id": "7934738904c27a27",
   "metadata": {},
   "source": [
    "# Simple visualization of neural network as a black box\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Draw the box\n",
    "box = plt.Rectangle((0.3, 0.2), 0.4, 0.6, fill=True, color='gray', alpha=0.7)\n",
    "ax.add_patch(box)\n",
    "\n",
    "# Add text\n",
    "plt.text(0.5, 0.5, 'Neural Network', ha='center', va='center', color='white', fontsize=15)\n",
    "plt.text(0.1, 0.5, 'Input\\nData', ha='center', va='center', fontsize=14)\n",
    "plt.text(0.9, 0.5, 'Output\\nPredictions', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Add arrows\n",
    "plt.arrow(0.2, 0.5, 0.08, 0, head_width=0.05, head_length=0.02, fc='blue', ec='blue', width=0.01)\n",
    "plt.arrow(0.7, 0.5, 0.08, 0, head_width=0.05, head_length=0.02, fc='green', ec='green', width=0.01)\n",
    "\n",
    "# Remove axes\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.title('Neural Network as a \"Black Box\"', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a59b6962d01d5432",
   "metadata": {},
   "source": [
    "## 2. Biological Inspiration\n",
    "\n",
    "Neural networks were inspired by how neurons work in the human brain. Understanding this biological connection helps grasp the fundamental concept.\n",
    "\n",
    "### The Human Brain\n",
    "- Contains approximately 86 billion neurons\n",
    "- Each neuron connects to thousands of others through synapses\n",
    "- Sends electrical signals when stimulated enough\n",
    "- Forms complex networks that enable learning and memory\n",
    "\n",
    "### The Artificial Neuron\n",
    "The artificial neuron (or \"perceptron\") mimics this behavior:\n",
    "- Receives input signals (like dendrites receive signals from other neurons)\n",
    "- Applies weights to these signals (like synaptic strengths)\n",
    "- Sums them up (like the cell body integrating signals)\n",
    "- Passes the sum through an activation function (like neurons firing when enough signals accumulate)\n",
    "- Produces an output (like the axon sending signals to other neurons)\n",
    "\n",
    "Let's see how the biological and artificial neurons compare:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f6bb478c0384133",
   "metadata": {},
   "source": [
    "# Create a side-by-side comparison of biological and artificial neurons\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Biological neuron (simplified)\n",
    "ax1.set_title('Biological Neuron', fontsize=16)\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Draw dendrites\n",
    "for i in range(8):\n",
    "    angle = np.pi * i / 4\n",
    "    ax1.plot([3 + 2 * np.cos(angle), 5], [5 + 2 * np.sin(angle), 5], 'b-', lw=2)\n",
    "\n",
    "# Draw cell body\n",
    "ax1.add_patch(plt.Circle((5, 5), 1.5, fill=True, color='blue', alpha=0.7))\n",
    "\n",
    "# Draw axon\n",
    "ax1.plot([6.5, 9], [5, 5], 'b-', lw=3)\n",
    "\n",
    "# Labels\n",
    "ax1.text(2, 5, 'Dendrites\\n(receive signals)', ha='center', va='center', fontsize=12)\n",
    "ax1.text(5, 5, 'Cell Body\\n(integrates signals)', ha='center', va='center', color='white', fontsize=12)\n",
    "ax1.text(9, 5, 'Axon\\n(sends output)', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Artificial neuron\n",
    "ax2.set_title('Artificial Neuron (Perceptron)', fontsize=16)\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.axis('off')\n",
    "\n",
    "# Draw inputs\n",
    "for i in range(4):\n",
    "    y_pos = 3 + i\n",
    "    ax2.text(1, y_pos, f'x{i+1}', ha='center', va='center', fontsize=12)\n",
    "    ax2.plot([1.5, 3], [y_pos, 5], 'r-', lw=1.5)\n",
    "    ax2.text(2.3, y_pos + 0.3, f'w{i+1}', ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "# Draw summation\n",
    "ax2.add_patch(plt.Circle((5, 5), 1.5, fill=True, color='green', alpha=0.7))\n",
    "ax2.text(5, 5, '∑', ha='center', va='center', color='white', fontsize=20)\n",
    "\n",
    "# Draw activation function\n",
    "ax2.add_patch(plt.Rectangle((7, 4), 1.5, 2, fill=True, color='purple', alpha=0.7))\n",
    "ax2.text(7.75, 5, '', ha='center', va='center', color='white', fontsize=16)\n",
    "\n",
    "# Draw output\n",
    "ax2.arrow(8.5, 5, 1, 0, head_width=0.3, head_length=0.3, fc='black', ec='black')\n",
    "ax2.text(9.75, 5, 'Output', ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4ddd8e2c43fbf98",
   "metadata": {},
   "source": [
    "## 3. Basic Components of Neural Networks\n",
    "\n",
    "A neural network consists of several key components:\n",
    "\n",
    "### Neurons\n",
    "- The basic computational units\n",
    "- Each neuron performs a simple calculation\n",
    "- Connected together to form a powerful system\n",
    "\n",
    "### Layers\n",
    "Neural networks are organized in layers:\n",
    "- **Input Layer**: Receives the raw data\n",
    "- **Hidden Layers**: Process the information (can be multiple)\n",
    "- **Output Layer**: Produces the final result\n",
    "\n",
    "### Weights and Biases\n",
    "- **Weights**: Determine the strength of connections between neurons\n",
    "- **Biases**: Allow neurons to shift their activation function\n",
    "\n",
    "### The mathematical representation\n",
    "\n",
    "For a single neuron with inputs $x₁, x₂, ..., xₙ$:\n",
    "1. Calculate weighted sum: $z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b$\n",
    "2. Apply activation function: $output = \\sigma(z)$\n",
    "\n",
    "Let's visualize a simple neural network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5f874b1c5725678",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper to evenly space neurons vertically\n",
    "def compute_layer_positions(num_neurons, total_height=6, margin=1):\n",
    "    if num_neurons == 1:\n",
    "        return [total_height / 2]\n",
    "    step = (total_height - 2 * margin) / (num_neurons - 1)\n",
    "    return [margin + i * step for i in range(num_neurons)]\n",
    "\n",
    "def plot_neural_network(num_input=3, num_hidden=4, num_output=2, figsize=(10, 6)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Compute y positions\n",
    "    total_height = 6\n",
    "    y_input = compute_layer_positions(num_input, total_height)\n",
    "    y_hidden = compute_layer_positions(num_hidden, total_height)\n",
    "    y_output = compute_layer_positions(num_output, total_height)\n",
    "\n",
    "    # Layer x positions\n",
    "    input_x, hidden_x, output_x = 0.5, 2.0, 3.5\n",
    "\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, total_height)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw neurons\n",
    "    input_neurons = []\n",
    "    for i, y in enumerate(y_input):\n",
    "        circle = plt.Circle((input_x, y), 0.2, color='blue', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(input_x, y, f'x{i+1}', ha='center', va='center', color='white', fontsize=12)\n",
    "        input_neurons.append((input_x, y))\n",
    "\n",
    "    hidden_neurons = []\n",
    "    for i,y in enumerate(y_hidden):\n",
    "        circle = plt.Circle((hidden_x, y), 0.2, color='green', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(hidden_x, y, f'h{i+1}', ha='center', va='center', color='white', fontsize=12)\n",
    "        hidden_neurons.append((hidden_x, y))\n",
    "\n",
    "    output_neurons = []\n",
    "    for i, y in enumerate(y_output):\n",
    "        circle = plt.Circle((output_x, y), 0.2, color='red', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(output_x, y, f'y{i+1}', ha='center', va='center', color='white', fontsize=12)\n",
    "        output_neurons.append((output_x, y))\n",
    "\n",
    "    # Draw connections\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            ax.plot([i_pos[0] + 0.2, h_pos[0] - 0.2], [i_pos[1], h_pos[1]], 'k-', alpha=0.3)\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            ax.plot([h_pos[0] + 0.2, o_pos[0] - 0.2], [h_pos[1], o_pos[1]], 'k-', alpha=0.3)\n",
    "\n",
    "    # Add layer labels above the topmost neuron\n",
    "    ax.text(input_x, max(y_input) + 0.5, 'Input Layer', ha='center', fontsize=14)\n",
    "    ax.text(hidden_x, max(y_hidden) + 0.5, 'Hidden Layer', ha='center', fontsize=14)\n",
    "    ax.text(output_x, max(y_output) + 0.5, 'Output Layer', ha='center', fontsize=14)\n",
    "\n",
    "    # Title and layout\n",
    "    plt.title('Simple Neural Network Architecture', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.85, bottom=0.05)\n",
    "\n",
    "    return fig, ax\n",
    "    \n",
    "# Create an interactive neural network visualization\n",
    "def interactive_nn_visualization(num_input, num_hidden, num_output):\n",
    "    plot_neural_network(num_input, num_hidden, num_output)\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.1)\n",
    "    plt.show()\n",
    "\n",
    "# Create a widget for interactive visualization\n",
    "interact(interactive_nn_visualization, \n",
    "         num_input=widgets.IntSlider(min=1, max=10, step=1, value=3, description='Input Neurons:'),\n",
    "         num_hidden=widgets.IntSlider(min=1, max=10, step=1, value=4, description='Hidden Neurons:'),\n",
    "         num_output=widgets.IntSlider(min=1, max=10, step=1, value=2, description='Output Neurons:'))\n",
    "\n",
    "# # Also show a fixed example\n",
    "# plot_neural_network(4, 5, 3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a46d48c0d11fe711",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "\n",
    "Activation functions are crucial components that introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
    "\n",
    "### Why do we need activation functions?\n",
    "\n",
    "Without activation functions, neural networks would only be able to learn linear relationships. No matter how many layers we add, we'd still only be able to represent linear transformations.\n",
    "\n",
    "### Common activation functions:\n",
    "\n",
    "1. **Sigmoid**: Squashes values between 0 and 1\n",
    "   - Formula: σ(x) = 1 / (1 + e^(-x))\n",
    "   - Good for binary classification output layers\n",
    "\n",
    "2. **Hyperbolic Tangent (tanh)**: Squashes values between -1 and 1\n",
    "   - Formula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "   - Similar to sigmoid but centered around 0\n",
    "\n",
    "3. **Rectified Linear Unit (ReLU)**: Returns x if positive, otherwise 0\n",
    "   - Formula: ReLU(x) = max(0, x)\n",
    "   - Most popular for hidden layers due to efficiency and reduced vanishing gradient problem\n",
    "\n",
    "4. **Leaky ReLU**: Similar to ReLU but allows small negative values\n",
    "   - Formula: Leaky ReLU(x) = max(0.01x, x)\n",
    "   - Addresses the \"dying ReLU\" problem\n",
    "\n",
    "Let's visualize these activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3f6340e507dd607",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Define x range\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "# Plot function\n",
    "def plot_activation(activation):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    title = \"Activation Function\"\n",
    "\n",
    "    if activation == 'all':\n",
    "        plt.plot(x, sigmoid(x), 'b-', label='Sigmoid', linewidth=2)\n",
    "        plt.plot(x, tanh(x), 'g-', label='Tanh', linewidth=2)\n",
    "        plt.plot(x, relu(x), 'r-', label='ReLU', linewidth=2)\n",
    "        plt.plot(x, leaky_relu(x), 'm-', label='Leaky ReLU (α=0.01)', linewidth=2)\n",
    "        title = \"All Activation Functions\"\n",
    "    else:\n",
    "        if activation == 'sigmoid':\n",
    "            plt.plot(x, sigmoid(x), 'b-', label='Sigmoid', linewidth=2)\n",
    "        elif activation == 'tanh':\n",
    "            plt.plot(x, tanh(x), 'g-', label='Tanh', linewidth=2)\n",
    "        elif activation == 'relu':\n",
    "            plt.plot(x, relu(x), 'r-', label='ReLU', linewidth=2)\n",
    "        elif activation == 'leaky_relu':\n",
    "            plt.plot(x, leaky_relu(x), 'm-', label='Leaky ReLU (α=0.01)', linewidth=2)\n",
    "        title = f\"{activation.replace('_', ' ').title()} Activation Function\"\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.ylim(-1.5, 5.5)\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.xlabel('Input (x)', fontsize=12)\n",
    "    plt.ylabel('Output f(x)', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create dropdown widget\n",
    "activation_dropdown = widgets.Dropdown(\n",
    "    options=['all', 'sigmoid', 'tanh', 'relu', 'leaky_relu'],\n",
    "    value='sigmoid',\n",
    "    description='Function:',\n",
    ")\n",
    "\n",
    "# Launch interactive widget\n",
    "interact(plot_activation, activation=activation_dropdown)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "951b2718522ed70d",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation\n",
    "\n",
    "Forward propagation is the process of passing input data through the neural network to get an output.\n",
    "\n",
    "### The step-by-step process:\n",
    "\n",
    "1. Input data enters the network through the input layer\n",
    "2. For each neuron in the first hidden layer:\n",
    "   - Calculate the weighted sum of inputs + bias\n",
    "   - Apply the activation function\n",
    "   - Pass the result to the next layer\n",
    "3. Repeat for each layer until the output layer is reached\n",
    "4. The final output is the network's prediction or classification\n",
    "\n",
    "Let's visualize the forward propagation process with a simple network:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4ee4c1ca9376c67",
   "metadata": {},
   "source": [
    "# Create a simple forward propagation visualization with summation and activation\n",
    "def create_forward_propagation_animation(num_frames=80):\n",
    "    # Create a simple network\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Layer positions\n",
    "    input_x, hidden_x, output_x = 0.5, 2.0, 3.5\n",
    "    \n",
    "    # Neuron positions\n",
    "    input_neurons = [(input_x, 1), (input_x, 2), (input_x, 3)]\n",
    "    hidden_neurons = [(hidden_x, 1), (hidden_x, 2), (hidden_x, 3)]\n",
    "    output_neurons = [(output_x, 1.5), (output_x, 2.5)]\n",
    "    \n",
    "    # Draw neurons with lowered opacity initially\n",
    "    input_circles = []\n",
    "    for x, y in input_neurons:\n",
    "        circle = plt.Circle((x, y), 0.2, fill=True, color='blue', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        input_circles.append(circle)\n",
    "        \n",
    "    hidden_circles = []\n",
    "    for x, y in hidden_neurons:\n",
    "        circle = plt.Circle((x, y), 0.2, fill=True, color='green', alpha=0.4)\n",
    "        ax.add_patch(circle)\n",
    "        hidden_circles.append(circle)\n",
    "        \n",
    "    output_circles = []\n",
    "    for x, y in output_neurons:\n",
    "        circle = plt.Circle((x, y), 0.2, fill=True, color='red', alpha=0.4)\n",
    "        ax.add_patch(circle)\n",
    "        output_circles.append(circle)\n",
    "    \n",
    "    # Add layer labels\n",
    "    ax.text(input_x, 3.7, 'Input Layer', ha='center', fontsize=14)\n",
    "    ax.text(hidden_x, 3.7, 'Hidden Layer', ha='center', fontsize=14)\n",
    "    ax.text(output_x, 3.7, 'Output Layer', ha='center', fontsize=14)\n",
    "    \n",
    "    # Initialize signals on connections\n",
    "    input_to_hidden_signals = []\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            signal = plt.Circle((i_pos[0], i_pos[1]), 0.05, color='purple', alpha=0)\n",
    "            ax.add_patch(signal)\n",
    "            input_to_hidden_signals.append((signal, i_pos, h_pos))\n",
    "    \n",
    "    hidden_to_output_signals = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            signal = plt.Circle((h_pos[0], h_pos[1]), 0.05, color='purple', alpha=0)\n",
    "            ax.add_patch(signal)\n",
    "            hidden_to_output_signals.append((signal, h_pos, o_pos))\n",
    "    \n",
    "    # Create connection lines (initially with low opacity)\n",
    "    input_to_hidden_lines = []\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            line, = ax.plot([i_pos[0], h_pos[0]], [i_pos[1], h_pos[1]], 'k-', alpha=0.2)\n",
    "            input_to_hidden_lines.append(line)\n",
    "    \n",
    "    hidden_to_output_lines = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            line, = ax.plot([h_pos[0], o_pos[0]], [h_pos[1], o_pos[1]], 'k-', alpha=0.2)\n",
    "            hidden_to_output_lines.append(line)\n",
    "    \n",
    "    # Create summation and activation symbols (initially invisible)\n",
    "    hidden_sum_symbols = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        sum_text = ax.text(h_pos[0], h_pos[1], \"g(Σ)\", ha='center', va='center', \n",
    "                          fontsize=14, fontweight='bold', color='purple', alpha=0)\n",
    "        hidden_sum_symbols.append(sum_text)\n",
    "    \n",
    "    output_sum_symbols = []\n",
    "    for o_pos in output_neurons:\n",
    "        sum_text = ax.text(o_pos[0], o_pos[1], \"g(Σ)\", ha='center', va='center', \n",
    "                          fontsize=14, fontweight='bold', color='purple', alpha=0)\n",
    "        output_sum_symbols.append(sum_text)\n",
    "    \n",
    "    # Create title\n",
    "    title = ax.text(2, 0.2, 'Forward Propagation Process', ha='center', fontsize=16)\n",
    "    \n",
    "    # Setup the animation\n",
    "    def update(frame):\n",
    "        # First phase: Input neurons activate (frames 0-9)\n",
    "        if frame < 10:\n",
    "            progress = frame / 9\n",
    "            \n",
    "            # Highlight input neurons\n",
    "            for circle in input_circles:\n",
    "                circle.set_alpha(0.7 + 0.3 * progress)\n",
    "                \n",
    "            title.set_text('Phase 1: Input Neurons Activate')\n",
    "                \n",
    "        # Second phase: signals travel to hidden layer (frames 10-29)\n",
    "        elif frame < 30:\n",
    "            progress = (frame - 10) / 19\n",
    "            \n",
    "            # Move signals from input to hidden\n",
    "            for signal, start, end in input_to_hidden_signals:\n",
    "                x = start[0] + progress * (end[0] - start[0])\n",
    "                y = start[1] + progress * (end[1] - start[1])\n",
    "                signal.set_center((x, y))\n",
    "                signal.set_alpha(0.8)\n",
    "                \n",
    "            # Highlight the connections\n",
    "            for line in input_to_hidden_lines:\n",
    "                line.set_alpha(0.2 + 0.6 * progress)\n",
    "                \n",
    "            title.set_text('Phase 2: Signals Travel to Hidden Layer')\n",
    "                \n",
    "        # Third phase: hidden neurons sum inputs (frames 30-39)\n",
    "        elif frame < 40:\n",
    "            progress = (frame - 30) / 9\n",
    "            \n",
    "            # Fade out signals\n",
    "            for signal, _, _ in input_to_hidden_signals:\n",
    "                signal.set_alpha(0.8 * (1 - progress))\n",
    "                \n",
    "            # Show summation symbols\n",
    "            for sum_text in hidden_sum_symbols:\n",
    "                sum_text.set_alpha(progress)\n",
    "                \n",
    "            title.set_text('Phase 3: Hidden Neurons Sum Inputs')\n",
    "                \n",
    "        # Fourth phase: hidden neurons activate (frames 40-49)\n",
    "        elif frame < 50:\n",
    "            progress = (frame - 40) / 9\n",
    "            \n",
    "            # Hide summation symbols\n",
    "            for sum_text in hidden_sum_symbols:\n",
    "                sum_text.set_alpha(1 - progress)\n",
    "                \n",
    "            # # Show activation process\n",
    "            # for act_text in hidden_activation_symbols:\n",
    "            #     act_text.set_alpha(progress)\n",
    "                \n",
    "            # Highlight hidden neurons\n",
    "            for circle in hidden_circles:\n",
    "                circle.set_alpha(0.4 + 0.6 * progress)\n",
    "                \n",
    "            title.set_text('Phase 4: Hidden Neurons Apply Activation Function')\n",
    "                \n",
    "        # Fifth phase: signals travel to output (frames 50-59)\n",
    "        elif frame < 60:\n",
    "            progress = (frame - 50) / 9\n",
    "            \n",
    "            # # Hide activation symbols\n",
    "            # for act_text in hidden_activation_symbols:\n",
    "            #     act_text.set_alpha(1 - progress)\n",
    "            # \n",
    "            # Move signals from hidden to output\n",
    "            for signal, start, end in hidden_to_output_signals:\n",
    "                x = start[0] + progress * (end[0] - start[0])\n",
    "                y = start[1] + progress * (end[1] - start[1])\n",
    "                signal.set_center((x, y))\n",
    "                signal.set_alpha(0.8)\n",
    "                \n",
    "            # Highlight connections\n",
    "            for line in hidden_to_output_lines:\n",
    "                line.set_alpha(0.2 + 0.6 * progress)\n",
    "                \n",
    "            title.set_text('Phase 5: Signals Travel to Output Layer')\n",
    "                \n",
    "        # Sixth phase: output neurons sum inputs (frames 60-69)\n",
    "        elif frame < 70:\n",
    "            progress = (frame - 60) / 9\n",
    "            \n",
    "            # Fade out signals\n",
    "            for signal, _, _ in hidden_to_output_signals:\n",
    "                signal.set_alpha(0.8 * (1 - progress))\n",
    "                \n",
    "            # Show summation symbols\n",
    "            for sum_text in output_sum_symbols:\n",
    "                sum_text.set_alpha(progress)\n",
    "                \n",
    "            title.set_text('Phase 6: Output Neurons Sum Inputs')\n",
    "                \n",
    "        # Final phase: output neurons activate (frames 70-79)\n",
    "        else:\n",
    "            progress = (frame - 70) / 9\n",
    "            \n",
    "            # Hide summation symbols\n",
    "            for sum_text in output_sum_symbols:\n",
    "                sum_text.set_alpha(1 - progress)\n",
    "            #     \n",
    "            # # Show activation process\n",
    "            # for act_text in output_activation_symbols:\n",
    "            #     act_text.set_alpha(progress)\n",
    "                \n",
    "            # Highlight output neurons\n",
    "            for circle in output_circles:\n",
    "                circle.set_alpha(0.4 + 0.6 * progress)\n",
    "                \n",
    "            title.set_text('Phase 7: Output Neurons Generate Predictions')\n",
    "        \n",
    "        # Collect all artists that need to be updated\n",
    "        artists = [title] + input_circles + hidden_circles + output_circles + input_to_hidden_lines + hidden_to_output_lines\n",
    "        artists += [signal for signal, _, _ in input_to_hidden_signals]\n",
    "        artists += [signal for signal, _, _ in hidden_to_output_signals]\n",
    "        artists += hidden_sum_symbols + output_sum_symbols\n",
    "        # artists += hidden_activation_symbols + output_activation_symbols\n",
    "        \n",
    "        return artists\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, interval=100, blit=False)\n",
    "    plt.close()  # Prevents duplicate display\n",
    "    return anim\n",
    "\n",
    "# Create and display the animation\n",
    "anim = create_forward_propagation_animation()\n",
    "HTML(anim.to_jshtml())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50e50d76fb8e2172",
   "metadata": {},
   "source": [
    "## 6. Training Neural Networks\n",
    "\n",
    "Training a neural network involves optimizing its weights to minimize the difference between predicted and actual outputs.\n",
    "\n",
    "### Key components of training:\n",
    "\n",
    "1. **Loss Function**: Measures how far the predictions are from the actual values\n",
    "   - **Mean Squared Error (MSE)**: For regression problems\n",
    "   - **Cross-Entropy Loss**: For classification problems\n",
    "\n",
    "2. **Backpropagation**: The algorithm that computes gradients \n",
    "   - Calculates how each weight contributes to the error\n",
    "   - Applies the chain rule of calculus to find gradients\n",
    "\n",
    "3. **Gradient Descent**: The optimization algorithm\n",
    "   - Updates weights in the opposite direction of the gradient\n",
    "   - Learning rate controls the step size\n",
    "\n",
    "### The training process:\n",
    "\n",
    "1. Forward Propagation: Make predictions\n",
    "2. Calculate Loss: Compare predictions with actual values\n",
    "3. Backpropagation: Compute gradients\n",
    "4. Update Weights: Adjust weights and biases\n",
    "5. Repeat: Iterate until convergence\n",
    "\n",
    "Let's visualize the gradient descent process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661be95d-d13c-4d86-af48-523bfead414f",
   "metadata": {},
   "source": [
    "### 🧠 What is Gradient Descent?\n",
    "\n",
    "Remember high school calculus? <br> \n",
    "We were  given a function $f(x) = x⁴ - 3x³ + …$ , and asked to analyze it — find maxima, minima, inflection points, and sketch the curve.\n",
    "\n",
    "**Step 1** was to take the derivative and solve **$f'(x) = 0$** . Why?<br>\n",
    "Because the derivative (or gradient) tells us **the slope of the function at each point**.\n",
    "\n",
    "At points where the gradient is zero, the slope is flat — meaning we’re at a local maximum or minimum.<br>\n",
    "But more importantly, if the gradient isn’t zero, it tells us **which direction to move x to decrease the function’s value**.\n",
    "\n",
    "<img src=\"data/presentation_files/GD_graph.png\" alt=\"Gradient descent\"/>\n",
    "\n",
    "**That’s the core idea behind gradient descent.**\n",
    "\n",
    "In machine learning, we:\n",
    "* Define a loss function L that measures how wrong our model is.\n",
    "* We compute the gradient of L with respect to the model parameters, at a specific sample or batch.\n",
    "* This gradient tells us how to adjust the parameters to make the loss a little smaller.\n",
    "\n",
    "* Repeat this over and over, and the loss goes down — step by step — until we reach a (local) minimum.\n",
    "\n",
    "#### That’s the learning."
   ]
  },
  {
   "cell_type": "code",
   "id": "98b3289a272f8894",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "# Create a visualization of gradient descent\n",
    "def plot_gradient_descent():\n",
    "    # Create a simple loss landscape\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = np.linspace(-2, 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = X**2 + Y**2  # Simple bowl-shaped loss function\n",
    "    \n",
    "    # Create a path of gradient descent\n",
    "    path_x = []\n",
    "    path_y = []\n",
    "    path_z = []\n",
    "    \n",
    "    # Starting point\n",
    "    current_x = 1.8\n",
    "    current_y = 1.8\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # Simulate gradient descent\n",
    "    for _ in range(20):\n",
    "        path_x.append(current_x)\n",
    "        path_y.append(current_y)\n",
    "        path_z.append(current_x**2 + current_y**2)\n",
    "        \n",
    "        # Gradient of our loss function (2x, 2y)\n",
    "        grad_x = 2 * current_x\n",
    "        grad_y = 2 * current_y\n",
    "        \n",
    "        # Update position (opposite direction of gradient)\n",
    "        current_x -= learning_rate * grad_x\n",
    "        current_y -= learning_rate * grad_y\n",
    "    \n",
    "    # Plot 3D surface\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot loss surface\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    # Plot path of gradient descent\n",
    "    ax.plot(path_x, path_y, path_z, 'ro-', markersize=7, linewidth=2, label='Gradient Descent Path')\n",
    "    \n",
    "    # Add a marker for the starting point\n",
    "    ax.plot([path_x[0]], [path_y[0]], [path_z[0]], 'ko', markersize=10, label='Starting Point')\n",
    "    \n",
    "    # Add a marker for the minimum\n",
    "    ax.plot([0], [0], [0], 'go', markersize=10, label='Global Minimum')\n",
    "    \n",
    "    ax.set_xlabel('Weight 1', fontsize=14)\n",
    "    ax.set_ylabel('Weight 2', fontsize=14)\n",
    "    ax.set_zlabel('Loss', fontsize=14)\n",
    "    ax.set_title('Gradient Descent Optimization', fontsize=16)\n",
    "    ax.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # # Also create a 2D contour plot for clarity\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # cp = plt.contourf(X, Y, Z, 20, cmap='viridis', alpha=0.7)\n",
    "    # plt.colorbar(cp)\n",
    "    \n",
    "    # # Plot the path of gradient descent\n",
    "    # plt.plot(path_x, path_y, 'ro-', markersize=7, linewidth=2, label='Gradient Descent Path')\n",
    "    # plt.plot(path_x[0], path_y[0], 'ko', markersize=10, label='Starting Point')\n",
    "    # plt.plot(0, 0, 'go', markersize=10, label='Global Minimum')\n",
    "    \n",
    "    # plt.title('Gradient Descent (Top View)', fontsize=16)\n",
    "    # plt.xlabel('Weight 1', fontsize=14)\n",
    "    # plt.ylabel('Weight 2', fontsize=14)\n",
    "    # plt.legend(fontsize=12)\n",
    "    # plt.grid(alpha=0.3)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "# Visualize gradient descent\n",
    "plot_gradient_descent()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53daa3ed-bc29-46f0-a795-6039ca72f641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Now let's create a detailed visualization of the backpropagation process\n",
    "def create_backpropagation_animation(num_frames=100):\n",
    "    # Create a simple network for visualization\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Layer positions\n",
    "    input_x, hidden_x, output_x = 1.0, 5.0, 9.0\n",
    "    \n",
    "    # Neuron positions\n",
    "    input_neurons = [(input_x, 1.5), (input_x, 4.5)]\n",
    "    hidden_neurons = [(hidden_x, 1), (hidden_x, 3), (hidden_x, 5)]\n",
    "    output_neurons = [(output_x, 3)]\n",
    "    \n",
    "    # Draw neurons\n",
    "    input_circles = []\n",
    "    for x, y in input_neurons:\n",
    "        circle = plt.Circle((x, y), 0.4, fill=True, color='blue', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        input_circles.append(circle)\n",
    "        \n",
    "    hidden_circles = []\n",
    "    for x, y in hidden_neurons:\n",
    "        circle = plt.Circle((x, y), 0.4, fill=True, color='green', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        hidden_circles.append(circle)\n",
    "        \n",
    "    output_circles = []\n",
    "    for x, y in output_neurons:\n",
    "        circle = plt.Circle((x, y), 0.4, fill=True, color='red', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        output_circles.append(circle)\n",
    "    \n",
    "    # Add layer labels\n",
    "    ax.text(input_x, 5.8, 'Input Layer', ha='center', fontsize=16)\n",
    "    ax.text(hidden_x, 5.8, 'Hidden Layer', ha='center', fontsize=16)\n",
    "    ax.text(output_x, 5.8, 'Output Layer', ha='center', fontsize=16)\n",
    "    \n",
    "    # Draw connections - Forward\n",
    "    forward_lines = []\n",
    "    ih_lines = []\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            line, = ax.plot([i_pos[0], h_pos[0]], [i_pos[1], h_pos[1]], 'k-', alpha=0.3, linewidth=1.5)\n",
    "            ih_lines.append(line)\n",
    "            forward_lines.append(line)\n",
    "    \n",
    "    ho_lines = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            line, = ax.plot([h_pos[0], o_pos[0]], [h_pos[1], o_pos[1]], 'k-', alpha=0.3, linewidth=1.5)\n",
    "            ho_lines.append(line)\n",
    "            forward_lines.append(line)\n",
    "    \n",
    "    # Add error and target symbols\n",
    "    error_symbol = ax.text(output_x + 1, output_neurons[0][1], \"Error/Loss \\n $(L=- Y \\log(\\hat{Y}))$\", color='red', fontsize=14, \n",
    "                          ha='center', va='center', visible=False)\n",
    "    target_symbol = ax.text(output_x + 1, output_neurons[0][1] + 0.8, \"Label $Y$\", color='green', fontsize=18, \n",
    "                           ha='center', va='center', visible=False)\n",
    "    \n",
    "    # Add prediction symbol\n",
    "    prediction_symbol = ax.text(output_x, output_neurons[0][1] - 0.8, \"Prediction $\\hat{Y}$\", color='blue', fontsize=18, \n",
    "                             ha='center', va='center', visible=False)\n",
    "    \n",
    "    # # Loss function symbol\n",
    "    # loss_box = plt.Rectangle((output_x + 0.8, output_neurons[0][1] - 0.5), 1.5, 1.0, \n",
    "    #                         fill=True, color='orange', alpha=0, edgecolor='black')\n",
    "    # ax.add_patch(loss_box)\n",
    "    # loss_text = ax.text(output_x + 1.5, output_neurons[0][1], \"Loss\\nFunction\", \n",
    "    #                    ha='center', va='center', color='black', fontsize=12, visible=False)\n",
    "    \n",
    "    # Backward lines (initially invisible)\n",
    "    backward_lines = []\n",
    "    oh_lines = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            line, = ax.plot([o_pos[0], h_pos[0]], [o_pos[1], h_pos[1]], 'r--', alpha=0, linewidth=4)\n",
    "            oh_lines.append(line)\n",
    "            backward_lines.append(line)\n",
    "    \n",
    "    hi_lines = []\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            line, = ax.plot([h_pos[0], i_pos[0]], [h_pos[1], i_pos[1]], 'r--', alpha=0, linewidth=4)\n",
    "            hi_lines.append(line)\n",
    "            backward_lines.append(line)\n",
    "    \n",
    "    # Gradient symbols (initially invisible)\n",
    "    gradient_symbols = []\n",
    "    for i, line in enumerate(backward_lines):\n",
    "        x = (line.get_xdata()[0] + line.get_xdata()[1]) / 2\n",
    "        y = (line.get_ydata()[0] + line.get_ydata()[1]) / 2\n",
    "        symbol = ax.text(x, y, \"∇L\", color='red', fontsize=14, fontweight='bold',\n",
    "                       ha='center', va='center', bbox=dict(facecolor='white', alpha=0.7), visible=False)\n",
    "        gradient_symbols.append(symbol)\n",
    "    \n",
    "    # Weight update symbols (initially invisible)\n",
    "    weight_update_symbols = []\n",
    "    for i, line in enumerate(forward_lines):\n",
    "        x = (line.get_xdata()[0] + line.get_xdata()[1]) / 2\n",
    "        y = (line.get_ydata()[0] + line.get_ydata()[1]) / 2\n",
    "        symbol = ax.text(x, y, \"w = w - η∇L\", color='purple', fontsize=14, fontweight='bold',\n",
    "                       ha='center', va='center', bbox=dict(facecolor='white', alpha=0.7), visible=False)\n",
    "        weight_update_symbols.append(symbol)\n",
    "    \n",
    "    # Chain rule symbol\n",
    "    chain_rule = ax.text(5, 0.8, \"Loss derivative: ∂L/∂w = ∂L/∂a × ∂a/∂z × ∂z/∂w (Chain Rule)\", \n",
    "                        ha='center', va='center', fontsize=14, color='purple', \n",
    "                        bbox=dict(facecolor='white', alpha=0.7), visible=False)\n",
    "    \n",
    "    # Add title\n",
    "    title = ax.text(5, 0.3, 'Backpropagation Process', ha='center', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Add explanation text box (initially invisible)\n",
    "    explanation = ax.text(5, 5.5, \"\", ha='center', va='center', fontsize=12,\n",
    "                        bbox=dict(facecolor='lightyellow', alpha=0.7), visible=False)\n",
    "    \n",
    "    # Initialize signals on connections (initially invisible)\n",
    "    input_to_hidden_signals = []\n",
    "    for i_pos in input_neurons:\n",
    "        for h_pos in hidden_neurons:\n",
    "            signal = plt.Circle((i_pos[0], i_pos[1]), 0.05, color='purple', alpha=0)\n",
    "            ax.add_patch(signal)\n",
    "            input_to_hidden_signals.append((signal, i_pos, h_pos))\n",
    "    \n",
    "    hidden_to_output_signals = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        for o_pos in output_neurons:\n",
    "            signal = plt.Circle((h_pos[0], h_pos[1]), 0.05, color='purple', alpha=0)\n",
    "            ax.add_patch(signal)\n",
    "            hidden_to_output_signals.append((signal, h_pos, o_pos))\n",
    "    \n",
    "    # Create summation and activation symbols (initially invisible)\n",
    "    hidden_sum_symbols = []\n",
    "    for h_pos in hidden_neurons:\n",
    "        sum_text = ax.text(h_pos[0], h_pos[1], \"g(Σ)\", ha='center', va='center', \n",
    "                          fontsize=20, fontweight='bold', color='purple', alpha=0)\n",
    "        hidden_sum_symbols.append(sum_text)\n",
    "    \n",
    "    output_sum_symbols = []\n",
    "    for o_pos in output_neurons:\n",
    "        sum_text = ax.text(o_pos[0], o_pos[1], \"g(Σ)\", ha='center', va='center', \n",
    "                          fontsize=20, fontweight='bold', color='purple', alpha=0)\n",
    "        output_sum_symbols.append(sum_text)\n",
    "    \n",
    "    # Animation function\n",
    "    def update(frame):\n",
    "        # Reset visibility of dynamic elements\n",
    "        error_symbol.set_visible(False)\n",
    "        target_symbol.set_visible(False)\n",
    "        prediction_symbol.set_visible(False)\n",
    "        # loss_box.set_alpha(0)\n",
    "        # loss_text.set_visible(False)\n",
    "        chain_rule.set_visible(False)\n",
    "        explanation.set_visible(False)\n",
    "        \n",
    "        for line in backward_lines:\n",
    "            line.set_alpha(0)\n",
    "        for symbol in gradient_symbols:\n",
    "            symbol.set_visible(False)\n",
    "        for symbol in weight_update_symbols:\n",
    "            symbol.set_visible(False)\n",
    "        \n",
    "        # Reset signal visibility and summation symbols\n",
    "        for signal, _, _ in input_to_hidden_signals + hidden_to_output_signals:\n",
    "            signal.set_alpha(0)\n",
    "        for sum_text in hidden_sum_symbols + output_sum_symbols:\n",
    "            sum_text.set_alpha(0)\n",
    "        \n",
    "        # Phase 1: Forward pass (frames 0-19)\n",
    "        if frame < 20:\n",
    "            percentage = frame / 19\n",
    "            title.set_text('Phase 1: Forward Propagation')\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Data flows through the network from inputs to outputs.\\nEach neuron computes a weighted sum of inputs, then applies an activation function.\")\n",
    "            \n",
    "            # First part: Input to hidden layer (frames 0-9)\n",
    "            if frame < 10:\n",
    "                sub_percentage = frame / 9\n",
    "                \n",
    "                # Highlight input neurons\n",
    "                for circle in input_circles:\n",
    "                    circle.set_alpha(0.7 + 0.3 * sub_percentage)\n",
    "                \n",
    "                # Move signals from input to hidden\n",
    "                for signal, start, end in input_to_hidden_signals:\n",
    "                    progress = sub_percentage\n",
    "                    x = start[0] + progress * (end[0] - start[0])\n",
    "                    y = start[1] + progress * (end[1] - start[1])\n",
    "                    signal.set_center((x, y))\n",
    "                    signal.set_alpha(0.8)\n",
    "                    \n",
    "                # Highlight input-hidden connections\n",
    "                for line in ih_lines:\n",
    "                    line.set_alpha(0.3 + 0.5 * sub_percentage)\n",
    "                \n",
    "            # Second part: Process in hidden layer and propagate to output (frames 10-19)\n",
    "            else:\n",
    "                sub_percentage = (frame - 10) / 9\n",
    "                \n",
    "                # First, show summation in hidden layer\n",
    "                if frame < 15:\n",
    "                    inner_progress = (frame - 10) / 4.5\n",
    "                    \n",
    "                    # Fade out input-hidden signals\n",
    "                    for signal, _, _ in input_to_hidden_signals:\n",
    "                        signal.set_alpha(0.8 * (1 - inner_progress))\n",
    "                    \n",
    "                    # Show summation symbols in hidden layer\n",
    "                    for sum_text in hidden_sum_symbols:\n",
    "                        sum_text.set_alpha(inner_progress)\n",
    "                    \n",
    "                    # Highlight hidden neurons\n",
    "                    for circle in hidden_circles:\n",
    "                        circle.set_alpha(0.7 + 0.3 * inner_progress)\n",
    "                \n",
    "                # Then propagate to output layer\n",
    "                else:\n",
    "                    inner_progress = (frame - 15) / 4.5\n",
    "                    \n",
    "                    # Hide summation symbols in hidden layer\n",
    "                    for sum_text in hidden_sum_symbols:\n",
    "                        sum_text.set_alpha(1 - inner_progress)\n",
    "                    \n",
    "                    # Move signals from hidden to output\n",
    "                    for signal, start, end in hidden_to_output_signals:\n",
    "                        progress = inner_progress\n",
    "                        x = start[0] + progress * (end[0] - start[0])\n",
    "                        y = start[1] + progress * (end[1] - start[1])\n",
    "                        signal.set_center((x, y))\n",
    "                        signal.set_alpha(0.8)\n",
    "                    \n",
    "                    # Highlight hidden-output connections\n",
    "                    for line in ho_lines:\n",
    "                        line.set_alpha(0.3 + 0.5 * inner_progress)\n",
    "                    \n",
    "                    # Highlight output neurons at the end\n",
    "                    for circle in output_circles:\n",
    "                        circle.set_alpha(0.7 + 0.3 * inner_progress)\n",
    "        \n",
    "        # Phase 2: Calculate loss (frames 20-34)\n",
    "        elif frame < 35:\n",
    "            title.set_text('Phase 2: Calculate Loss (Prediction vs Target)')\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Compare network prediction with target value.\\nLoss function measures how far off the prediction is from the target.\")\n",
    "            \n",
    "            # Fade out signals from hidden to output\n",
    "            if frame < 23:\n",
    "                fadeout_progress = (frame - 20) / 2.5\n",
    "                for signal, _, _ in hidden_to_output_signals:\n",
    "                    signal.set_alpha(0.8 * (1 - fadeout_progress))\n",
    "                \n",
    "                # Show summation in output\n",
    "                for sum_text in output_sum_symbols:\n",
    "                    sum_text.set_alpha(fadeout_progress)\n",
    "            else:\n",
    "                # Hide output summation\n",
    "                for sum_text in output_sum_symbols:\n",
    "                    sum_text.set_alpha(0)\n",
    "            \n",
    "            # Show prediction and target\n",
    "            if frame >= 25:\n",
    "                prediction_symbol.set_visible(True)\n",
    "                target_symbol.set_visible(True)\n",
    "            \n",
    "            # Pulse the output neuron to indicate prediction\n",
    "            pulse = 0.7 + 0.3 * np.sin((frame - 20) * np.pi / 4.5)\n",
    "            for circle in output_circles:\n",
    "                circle.set_alpha(pulse)\n",
    "            \n",
    "            # Gradually show the loss calculation\n",
    "            if frame >= 28:\n",
    "                # sub_percentage = (frame - 28) / 6\n",
    "                # # loss_box.set_alpha(sub_percentage * 0.5)\n",
    "                # # loss_text.set_visible(True)\n",
    "                \n",
    "                if frame >= 32:\n",
    "                    error_symbol.set_visible(True)\n",
    "        \n",
    "        # Phase 3: Understanding the chain rule (frames 35-44)\n",
    "        elif frame < 45:\n",
    "            title.set_text('Phase 3: Applying Chain Rule for Gradients')\n",
    "            prediction_symbol.set_visible(True)\n",
    "            target_symbol.set_visible(True)\n",
    "            error_symbol.set_visible(True)\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Backpropagation uses the chain rule from calculus to compute gradients.\\nThese gradients show how each weight contributes to the final error.\")\n",
    "            \n",
    "            # Show the chain rule formula\n",
    "            chain_rule.set_visible(True)\n",
    "            \n",
    "            # Pulse the output neuron\n",
    "            pulse = 0.7 + 0.3 * np.sin((frame - 35) * np.pi / 4.5)\n",
    "            for circle in output_circles:\n",
    "                circle.set_alpha(pulse)\n",
    "            \n",
    "        # Phase 4: Backpropagate to hidden layer (frames 45-59)\n",
    "        elif frame < 60:\n",
    "            title.set_text('Phase 4: Backpropagate Gradients to Hidden Layer')\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Gradients flow backward from output to hidden layer.\\nThese tell us how much each hidden neuron contributed to the error.\")\n",
    "            \n",
    "            percentage = (frame - 45) / 14\n",
    "            # Gradually show backward propagation lines from output to hidden\n",
    "            for line in oh_lines:\n",
    "                line.set_alpha(min(1.0, percentage * 2))\n",
    "            \n",
    "            # Show gradient symbols on backprop lines\n",
    "            if frame >= 52:\n",
    "                for i, symbol in enumerate(gradient_symbols[:len(oh_lines)]):\n",
    "                    if i % 3 == (frame - 52) % 3:  # Show them gradually\n",
    "                        symbol.set_visible(True)\n",
    "            \n",
    "            # Pulse hidden neurons as they receive gradients\n",
    "            if frame >= 55:\n",
    "                pulse = 0.7 + 0.3 * np.sin((frame - 55) * np.pi / 4)\n",
    "                for circle in hidden_circles:\n",
    "                    circle.set_alpha(pulse)\n",
    "        \n",
    "        # Phase 5: Backpropagate to input layer (frames 60-74)\n",
    "        elif frame < 75:\n",
    "            title.set_text('Phase 5: Backpropagate Gradients to Input Layer')\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Gradients continue flowing to the input layer.\\nNeural networks use this multi-layer gradient flow to update all weights properly.\")\n",
    "            \n",
    "            percentage = (frame - 60) / 14\n",
    "            # Keep output to hidden backprop lines visible\n",
    "            for line in oh_lines:\n",
    "                line.set_alpha(1.0)\n",
    "                \n",
    "            # Keep gradient symbols visible\n",
    "            for symbol in gradient_symbols[:len(oh_lines)]:\n",
    "                symbol.set_visible(True)\n",
    "                \n",
    "            # Gradually show backward propagation lines from hidden to input\n",
    "            for line in hi_lines:\n",
    "                line.set_alpha(min(1.0, percentage * 2))\n",
    "                \n",
    "            # Show gradient symbols on input backprop lines\n",
    "            if frame >= 67:\n",
    "                for i, symbol in enumerate(gradient_symbols[len(oh_lines):]):\n",
    "                    if i % 3 == (frame - 67) % 3:  # Show them gradually\n",
    "                        symbol.set_visible(True)\n",
    "        \n",
    "        # Phase 6: Update weights (frames 75-99)\n",
    "        else:\n",
    "            title.set_text('Phase 6: Update Weights Using Gradients')\n",
    "            explanation.set_visible(True)\n",
    "            explanation.set_text(\"Weights are updated in the direction that reduces error: w = w - η∇L\\nLearning rate (η) controls how large each adjustment is.\")\n",
    "            \n",
    "            # Show all backprop lines\n",
    "            for line in backward_lines:\n",
    "                line.set_alpha(0.7)\n",
    "                \n",
    "            # Display all gradient symbols faded\n",
    "            for symbol in gradient_symbols:\n",
    "                symbol.set_visible(True)\n",
    "                symbol.set_alpha(0.5)\n",
    "            \n",
    "            # Show weight update symbols one by one\n",
    "            update_idx = int((frame - 75) / 2) % len(weight_update_symbols)\n",
    "            weight_update_symbols[update_idx].set_visible(True)\n",
    "            \n",
    "            # Flash weights being updated - ensure alpha values stay in safe range\n",
    "            flash_alpha = 0.3 + 0.5 * np.sin((frame - 75) * np.pi / 5)\n",
    "            safe_alpha = max(0.1, min(0.9, flash_alpha))\n",
    "            \n",
    "            if (frame - 75) >= len(weight_update_symbols) * 2:\n",
    "                # Flash all connections simultaneously in later frames\n",
    "                for line in forward_lines:\n",
    "                    line.set_alpha(safe_alpha)\n",
    "            else:\n",
    "                # Focus on one connection at a time in earlier frames\n",
    "                i = update_idx\n",
    "                if i < len(forward_lines):\n",
    "                    forward_lines[i].set_alpha(0.9)\n",
    "        \n",
    "        artists = [title, explanation, error_symbol, target_symbol, prediction_symbol, chain_rule]\n",
    "                  # loss_box, loss_text, \n",
    "        artists += forward_lines + backward_lines\n",
    "        artists += gradient_symbols + weight_update_symbols\n",
    "        artists += input_circles + hidden_circles + output_circles\n",
    "        artists += [signal for signal, _, _ in input_to_hidden_signals + hidden_to_output_signals]\n",
    "        artists += hidden_sum_symbols + output_sum_symbols\n",
    "        \n",
    "        return artists\n",
    "    \n",
    "    # Create the animation\n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, interval=100, blit=True)\n",
    "    plt.close()  # Prevents duplicate display\n",
    "    return anim\n",
    "\n",
    "# Create and display the backpropagation animation\n",
    "backprop_anim = create_backpropagation_animation()\n",
    "HTML(backprop_anim.to_jshtml())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73e8d9d867155d74",
   "metadata": {},
   "source": [
    "## 7. Simple Neural Network Example\n",
    "\n",
    "Let's understand how a neural network works by implementing a very simple example from scratch. We'll create a network that can learn to classify points in a 2D plane.\n",
    "\n",
    "The network will:\n",
    "1. Take 2 input features (x and y coordinates)\n",
    "2. Have 1 hidden layer with 4 neurons\n",
    "3. Output a binary classification (0 or 1)\n",
    "\n",
    "First, let's create some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fead2d0f86f9dee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Generate a simple dataset\n",
    "def generate_data(n_samples=100, noise=0.1, random_state=42):\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "    return X, y\n",
    "\n",
    "# Create data\n",
    "X, y = generate_data(200)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='red', alpha=0.7, label='Class 1')\n",
    "plt.title('Sample Data: Two Moons', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be2c95870658c93a",
   "metadata": {},
   "source": [
    "# Now let's implement a simple neural network from scratch\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Backpropagation\n",
    "        self.output_error = y.reshape(-1, 1) - output\n",
    "        self.output_delta = self.output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        self.hidden_error = self.output_delta.dot(self.W2.T)\n",
    "        self.hidden_delta = self.hidden_error * self.sigmoid_derivative(self.a1)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 += self.a1.T.dot(self.output_delta) * learning_rate\n",
    "        self.b2 += np.sum(self.output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.W1 += X.T.dot(self.hidden_delta) * learning_rate\n",
    "        self.b1 += np.sum(self.hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean(np.square(y.reshape(-1, 1) - output))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) >= 0.5).astype(int)\n",
    "    \n",
    "# Create and train our neural network\n",
    "NUM_OF_EPOCHS = 5000\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "losses = nn.train(X_train, y_train, epochs=NUM_OF_EPOCHS, learning_rate=0.1)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Time', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = nn.predict(X_test).flatten()\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c518404ed844b42a",
   "metadata": {},
   "source": [
    "# Let's visualize how our model classifies the data\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values with some padding\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    # Create a meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Predict for each point in the meshgrid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', alpha=0.8, label='Class 0')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], color='red', alpha=0.8, label='Class 1')\n",
    "    \n",
    "    plt.title('Decision Boundary', fontsize=16)\n",
    "    plt.xlabel('Feature 1', fontsize=14)\n",
    "    plt.ylabel('Feature 2', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the decision boundary\n",
    "plot_decision_boundary(nn, X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfc70ab432f56b54",
   "metadata": {},
   "source": [
    "## 8. Visualizing a Neural Network\n",
    "\n",
    "Let's create a more detailed visualization of a neural network and how it processes data. We'll see how the data flows through the network layers.\n",
    "\n",
    "For this visualization, we'll use a simpler dataset that's easier to understand:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0d954ed65b5d6f4",
   "metadata": {},
   "source": [
    "# Create a simple dataset with two clusters\n",
    "X_simple, y_simple = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42)\n",
    "\n",
    "# Plot the simple dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], color='blue', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], color='red', alpha=0.7, label='Class 1')\n",
    "plt.title('Simple Dataset: Two Clusters', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# We'll use this data to train a very simple network and visualize its activations\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize data for better visualization\n",
    "X_train_simple = (X_train_simple - X_train_simple.mean(axis=0)) / X_train_simple.std(axis=0)\n",
    "X_test_simple = (X_test_simple - X_test_simple.mean(axis=0)) / X_test_simple.std(axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32a8769ec7bee5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Create a simple neural network with visualization capabilities\n",
    "class VisualNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with fixed random values for consistent visualization\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Store all intermediate values for visualization\n",
    "        self.X = X\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) >= 0.5).astype(int)\n",
    "    \n",
    "    def visualize_network(self, sample_idx=0):\n",
    "        \"\"\"Visualize the network with activations for a specific sample\"\"\"\n",
    "        # Ensure we have run forward pass\n",
    "        if not hasattr(self, 'X'):\n",
    "            raise ValueError(\"Run forward pass first\")\n",
    "            \n",
    "        # Get a single sample\n",
    "        x = self.X[sample_idx:sample_idx+1]\n",
    "        \n",
    "        # Run forward pass for this sample if not already done\n",
    "        self.forward(x)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 6)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Layer positions\n",
    "        input_x, hidden_x, output_x = 1.0, 5.0, 9.0\n",
    "        \n",
    "        # Place neurons\n",
    "        input_neurons = []\n",
    "        for i in range(2):  # 2 input features\n",
    "            y_pos = 3 - i\n",
    "            # Create neuron with activation value from input\n",
    "            circle = plt.Circle((input_x, y_pos), 0.4, fill=True, \n",
    "                                color='blue', alpha=0.7)\n",
    "            ax.add_patch(circle)\n",
    "            # Display the value\n",
    "            ax.text(input_x, y_pos, f'{x[0, i]:.2f}', ha='center', va='center', \n",
    "                    color='white', fontsize=12)\n",
    "            input_neurons.append((input_x, y_pos))\n",
    "        \n",
    "        # Add input labels\n",
    "        ax.text(input_x, 4.2, 'Input Layer', ha='center', fontsize=16)\n",
    "        ax.text(input_x - 0.8, 3, 'x₁', ha='center', va='center', fontsize=14)\n",
    "        ax.text(input_x - 0.8, 2, 'x₂', ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        # Hidden layer neurons\n",
    "        hidden_neurons = []\n",
    "        for i in range(3):  # 3 hidden neurons\n",
    "            y_pos = 4 - i\n",
    "            # Map activation to color intensity\n",
    "            activation = self.a1[0, i]\n",
    "            # Create neuron with activation-based color\n",
    "            circle = plt.Circle((hidden_x, y_pos), 0.4, fill=True, \n",
    "                                color='green', alpha=0.4 + 0.5 * activation)\n",
    "            ax.add_patch(circle)\n",
    "            # Display the activation value\n",
    "            ax.text(hidden_x, y_pos, f'{activation:.2f}', ha='center', va='center', \n",
    "                    color='white', fontsize=12)\n",
    "            hidden_neurons.append((hidden_x, y_pos))\n",
    "        \n",
    "        # Add hidden layer label\n",
    "        ax.text(hidden_x, 5.2, 'Hidden Layer', ha='center', fontsize=16)\n",
    "        \n",
    "        # Output layer neurons\n",
    "        output_neurons = []\n",
    "        for i in range(1):  # 1 output neuron\n",
    "            y_pos = 3\n",
    "            # Get the prediction\n",
    "            prediction = self.a2[0, i]\n",
    "            # Create neuron with activation-based color\n",
    "            circle = plt.Circle((output_x, y_pos), 0.4, fill=True, \n",
    "                                color='red', alpha=0.4 + 0.5 * prediction)\n",
    "            ax.add_patch(circle)\n",
    "            # Display the prediction\n",
    "            ax.text(output_x, y_pos, f'{prediction:.2f}', ha='center', va='center', \n",
    "                    color='white', fontsize=12)\n",
    "            output_neurons.append((output_x, y_pos))\n",
    "            \n",
    "            # Add class prediction\n",
    "            predicted_class = \"Class 1\" if prediction >= 0.5 else \"Class 0\"\n",
    "            ax.text(output_x, y_pos - 1, f'Prediction:\\n{predicted_class}', \n",
    "                    ha='center', va='center', fontsize=14)\n",
    "        \n",
    "        # Add output layer label\n",
    "        ax.text(output_x, 5.2, 'Output Layer', ha='center', fontsize=16)\n",
    "        \n",
    "        # Connect input to hidden layer with weighted connections\n",
    "        for i, i_pos in enumerate(input_neurons):\n",
    "            for j, h_pos in enumerate(hidden_neurons):\n",
    "                # Get the weight for this connection\n",
    "                weight = self.W1[i, j]\n",
    "                # Set line width based on weight magnitude\n",
    "                lw = 0.5 + 2 * abs(weight)\n",
    "                # Set color based on weight sign\n",
    "                color = 'red' if weight < 0 else 'green'\n",
    "                # Set alpha based on weight magnitude\n",
    "                alpha = 0.3 + 0.7 * abs(weight) / max(0.1, abs(self.W1).max())\n",
    "                # Draw the connection\n",
    "                ax.plot([i_pos[0] + 0.4, h_pos[0] - 0.4], [i_pos[1], h_pos[1]], \n",
    "                        '-', color=color, linewidth=lw, alpha=alpha)\n",
    "                # Add weight label\n",
    "                mid_x = (i_pos[0] + h_pos[0]) / 2\n",
    "                mid_y = (i_pos[1] + h_pos[1]) / 2\n",
    "                ax.text(mid_x, mid_y, f'{weight:.2f}', ha='center', va='center', \n",
    "                        fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Connect hidden to output layer\n",
    "        for i, h_pos in enumerate(hidden_neurons):\n",
    "            for j, o_pos in enumerate(output_neurons):\n",
    "                # Get the weight for this connection\n",
    "                weight = self.W2[i, j]\n",
    "                # Set line width based on weight magnitude\n",
    "                lw = 0.5 + 2 * abs(weight)\n",
    "                # Set color based on weight sign\n",
    "                color = 'red' if weight < 0 else 'green'\n",
    "                # Set alpha based on weight magnitude\n",
    "                alpha = 0.3 + 0.7 * abs(weight) / max(0.1, abs(self.W2).max())\n",
    "                # Draw the connection\n",
    "                ax.plot([h_pos[0] + 0.4, o_pos[0] - 0.4], [h_pos[1], o_pos[1]], \n",
    "                        '-', color=color, linewidth=lw, alpha=alpha)\n",
    "                # Add weight label\n",
    "                mid_x = (h_pos[0] + o_pos[0]) / 2\n",
    "                mid_y = (h_pos[1] + o_pos[1]) / 2\n",
    "                ax.text(mid_x, mid_y, f'{weight:.2f}', ha='center', va='center', \n",
    "                        fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Add title\n",
    "        ax.set_title('Neural Network with Activations and Weights', fontsize=18)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='blue', alpha=0.7, label='Input Neurons'),\n",
    "            Patch(facecolor='green', alpha=0.7, label='Hidden Neurons'),\n",
    "            Patch(facecolor='red', alpha=0.7, label='Output Neurons'),\n",
    "            Patch(facecolor='green', edgecolor='black', label='Positive Weight'),\n",
    "            Patch(facecolor='red', edgecolor='black', label='Negative Weight')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper center', \n",
    "                  bbox_to_anchor=(0.5, -0.05), ncol=3, fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and test our visual neural network\n",
    "visual_nn = VisualNN(input_size=2, hidden_size=3, output_size=1)\n",
    "\n",
    "# Forward pass on the whole dataset\n",
    "predictions = visual_nn.forward(X_train_simple)\n",
    "\n",
    "# Visualize the network for the first sample\n",
    "visual_nn.visualize_network(sample_idx=0)\n",
    "\n",
    "# Let's also see the network for a sample from the other class\n",
    "other_class_idx = np.where(y_train_simple != y_train_simple[0])[0][0]\n",
    "visual_nn.visualize_network(sample_idx=other_class_idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "906aa910de7712e",
   "metadata": {},
   "source": [
    "## 9. Applications and Use Cases\n",
    "\n",
    "Neural networks are versatile tools that can be applied to a wide range of problems:\n",
    "\n",
    "### Image Classification\n",
    "- Identifying objects in images\n",
    "- Medical image analysis\n",
    "- Facial recognition\n",
    "- Self-driving cars vision\n",
    "\n",
    "### Natural Language Processing\n",
    "- Machine translation\n",
    "- Sentiment analysis\n",
    "- Text generation\n",
    "- Chatbots and virtual assistants\n",
    "\n",
    "### Speech Recognition\n",
    "- Voice assistants\n",
    "- Call center automation\n",
    "- Transcription services\n",
    "- Language identification\n",
    "\n",
    "### Time Series Analysis\n",
    "- Stock price prediction\n",
    "- Weather forecasting\n",
    "- Energy demand prediction\n",
    "- Anomaly detection\n",
    "\n",
    "### Recommender Systems\n",
    "- Product recommendations\n",
    "- Content suggestions\n",
    "- Personalized marketing\n",
    "- Movie and music recommendations\n",
    "\n",
    "### Game Playing\n",
    "- Chess and Go champions\n",
    "- Video game AI\n",
    "- Strategy optimization\n",
    "- Reinforcement learning agents\n",
    "\n",
    "Let's visualize some of these applications:"
   ]
  },
  {
   "cell_type": "code",
   "id": "817a5249d006b767",
   "metadata": {},
   "source": [
    "# Visualize various neural network applications\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. Image Classification\n",
    "axes[0].set_title('Image Classification', fontsize=14)\n",
    "axes[0].text(0.5, 0.5, 'Image → CNN → [Dog, Cat, Bird, ...]', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 2. Natural Language Processing\n",
    "axes[1].set_title('Natural Language Processing', fontsize=14)\n",
    "axes[1].text(0.5, 0.5, 'Text → RNN/Transformer → Intent/Meaning', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 3. Time Series Analysis\n",
    "axes[2].set_title('Time Series Analysis', fontsize=14)\n",
    "# Create a simple time series\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x) + np.random.normal(0, 0.1, size=100)\n",
    "axes[2].plot(x, y, 'b-')\n",
    "axes[2].plot(x, np.sin(x), 'r--')\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_ylabel('Value')\n",
    "\n",
    "# 4. Recommender Systems\n",
    "axes[3].set_title('Recommender Systems', fontsize=14)\n",
    "# Create a user-item matrix visualization\n",
    "user_item = np.random.randint(0, 5, size=(10, 15))\n",
    "user_item[user_item == 0] = np.nan\n",
    "im = axes[3].imshow(user_item, cmap='YlGnBu')\n",
    "axes[3].set_xticks([])\n",
    "axes[3].set_yticks([])\n",
    "axes[3].set_xlabel('Items')\n",
    "axes[3].set_ylabel('Users')\n",
    "\n",
    "# 5. Reinforcement Learning\n",
    "axes[4].set_title('Reinforcement Learning', fontsize=14)\n",
    "# Create a simple maze-like environment\n",
    "grid = np.ones((10, 10))\n",
    "grid[2:8, 2] = 0\n",
    "grid[2, 2:8] = 0\n",
    "grid[2:8, 7] = 0\n",
    "grid[7, 2:8] = 0\n",
    "grid[4, 4:7] = 0\n",
    "grid[5, 4] = 0\n",
    "grid[2, 2] = 2  # Start\n",
    "grid[7, 7] = 3  # Goal\n",
    "cmap = plt.cm.colors.ListedColormap(['white', 'black', 'green', 'red'])\n",
    "axes[4].imshow(grid, cmap=cmap)\n",
    "axes[4].set_xticks([])\n",
    "axes[4].set_yticks([])\n",
    "\n",
    "# 6. Speech Recognition\n",
    "axes[5].set_title('Speech Recognition', fontsize=14)\n",
    "# Create a simple waveform and spectrogram\n",
    "t = np.linspace(0, 10, 1000)\n",
    "audio = np.sin(2 * np.pi * 1 * t) + 0.5 * np.sin(2 * np.pi * 2 * t) + 0.2 * np.sin(2 * np.pi * 4 * t)\n",
    "audio += np.random.normal(0, 0.1, size=1000)\n",
    "axes[5].plot(t, audio, 'k-', linewidth=0.5)\n",
    "axes[5].set_xticks([])\n",
    "axes[5].set_yticks([])\n",
    "axes[5].set_xlabel('Time')\n",
    "axes[5].text(5, 0, 'Audio → NN → Text', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a950491c292c646",
   "metadata": {},
   "source": [
    "## 10. Hands-on Exercise\n",
    "\n",
    "Now that you understand the basics of neural networks, let's try a hands-on exercise. \n",
    "\n",
    "In this exercise, you'll create a simple neural network to classify two types of data points.\n",
    "\n",
    "### Task:\n",
    "1. Create a dataset with two classes\n",
    "2. Build a simple neural network with one hidden layer\n",
    "3. Train the network\n",
    "4. Visualize the decision boundary\n",
    "5. Try changing the network architecture and see how it affects the results\n",
    "\n",
    "Here's some starter code to help you get going:"
   ]
  },
  {
   "cell_type": "code",
   "id": "48f255bfd42c9cce",
   "metadata": {},
   "source": [
    "# Exercise: Build your own neural network\n",
    "\n",
    "# Step 1: Create a dataset\n",
    "def create_dataset(n_samples=300, noise=0.2, random_state=None):\n",
    "    \"\"\"Create a dataset with two interleaving half circles\"\"\"\n",
    "    from sklearn.datasets import make_moons\n",
    "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "    return X, y\n",
    "\n",
    "# Step 2: Define a class for our neural network\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize random weights and biases\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Backpropagation\n",
    "        self.output_error = y.reshape(-1, 1) - output\n",
    "        self.output_delta = self.output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        self.hidden_error = self.output_delta.dot(self.W2.T)\n",
    "        self.hidden_delta = self.hidden_error * self.sigmoid_derivative(self.a1)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 += self.a1.T.dot(self.output_delta) * learning_rate\n",
    "        self.b2 += np.sum(self.output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.W1 += X.T.dot(self.hidden_delta) * learning_rate\n",
    "        self.b1 += np.sum(self.hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean(np.square(y.reshape(-1, 1) - output))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) >= 0.5).astype(int)\n",
    "\n",
    "# Step 3: Visualize decision boundaries\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values with some padding\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    # Create a meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Predict for each point in the meshgrid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', alpha=0.8, label='Class 0')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], color='red', alpha=0.8, label='Class 1')\n",
    "    \n",
    "    plt.title('Decision Boundary', fontsize=16)\n",
    "    plt.xlabel('Feature 1', fontsize=14)\n",
    "    plt.ylabel('Feature 2', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 4: Run the experiment\n",
    "# Create a dataset\n",
    "X, y = create_dataset(random_state=42)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='blue', alpha=0.7, label='Class 0')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='red', alpha=0.7, label='Class 1')\n",
    "plt.title('Training Data: Two Moons', fontsize=16)\n",
    "plt.xlabel('Feature 1', fontsize=14)\n",
    "plt.ylabel('Feature 2', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create the neural network\n",
    "# Try different hidden layer sizes! (e.g., 3, 5, 10, 20)\n",
    "nn = MyNeuralNetwork(input_size=2, hidden_size=5, output_size=1)\n",
    "\n",
    "# Train the network\n",
    "# Try different learning rates and epochs!\n",
    "losses = nn.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Time', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the decision boundary\n",
    "plot_decision_boundary(nn, X, y)\n",
    "\n",
    "# Step 5: Experiment!\n",
    "# TODO: Try modifying the neural network architecture or hyperparameters\n",
    "# Here are some ideas:\n",
    "# - Change the number of hidden layers and neurons\n",
    "# - Try different activation functions (ReLU, tanh)\n",
    "# - Adjust the learning rate\n",
    "# - Use different datasets\n",
    "# - Add more features to the input data\n",
    "\n",
    "# EXERCISE: Try with different hidden layer sizes\n",
    "print(\"Experiment with hidden layer size = 3:\")\n",
    "nn_small = MyNeuralNetwork(input_size=2, hidden_size=3, output_size=1)\n",
    "nn_small.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "plot_decision_boundary(nn_small, X, y)\n",
    "\n",
    "print(\"Experiment with hidden layer size = 10:\")\n",
    "nn_large = MyNeuralNetwork(input_size=2, hidden_size=10, output_size=1)\n",
    "nn_large.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "plot_decision_boundary(nn_large, X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5c78542549d0a1e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the fascinating world of neural networks from the ground up:\n",
    "\n",
    "1. **Introduction to Neural Networks**: We learned that neural networks are computational models inspired by the human brain, designed to recognize patterns in data.\n",
    "\n",
    "2. **Biological Inspiration**: We saw how artificial neurons mimic biological neurons, with inputs, weights, and activation functions.\n",
    "\n",
    "3. **Basic Components**: We explored the architecture of neural networks, including input layers, hidden layers, output layers, weights, and biases.\n",
    "\n",
    "4. **Activation Functions**: We examined various activation functions like sigmoid, tanh, and ReLU that introduce non-linearity into networks.\n",
    "\n",
    "5. **Forward Propagation**: We visualized how data flows through a neural network from input to output.\n",
    "\n",
    "6. **Simple Neural Network Example**: We built a neural network from scratch and trained it on a classification task.\n",
    "\n",
    "7. **Training**: We explored how neural networks learn through backpropagation and gradient descent.\n",
    "\n",
    "8. **Visualizing Neural Networks**: We created detailed visualizations of neural networks in action.\n",
    "\n",
    "9. **Applications**: We surveyed the diverse applications of neural networks in image recognition, NLP, time series analysis, and more.\n",
    "\n",
    "10. **Hands-on Exercise**: We put our knowledge into practice by building and experimenting with neural networks.\n",
    "\n",
    "Neural networks have revolutionized artificial intelligence and continue to drive innovations across virtually every industry. The concepts covered in this notebook provide a foundation for understanding more complex architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers.\n",
    "\n",
    "As you continue your journey in machine learning, remember that neural networks are powerful tools, but they're just one part of the machine learning ecosystem. Understanding when and how to apply them effectively is just as important as knowing how they work."
   ]
  },
  {
   "cell_type": "code",
   "id": "ecc8cbdb81d7db32",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5726954d3cea3aa",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Workshop)",
   "language": "python",
   "name": "mlworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
