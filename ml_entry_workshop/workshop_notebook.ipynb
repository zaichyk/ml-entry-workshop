{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1f6e0a96ffe651",
   "metadata": {},
   "source": [
    "# Welcome to the ML-Entry Workshop! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049c57f-63b2-43ab-8977-864ebfb548a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What We‚Äôll Cover:\n",
    "- Introduction\n",
    "- How does a Data Science project look?\n",
    "- How do we choose a model and train it?\n",
    "- Hands-on experience: Building Tinder! Create a dataset, and train a model to solve a real-world problem.\n",
    "- Theory Keyconcepts: Hypothesis set, Train vs Validation/Test, loss function, Backpropagtion, CNN...\n",
    "- Take it further: recommended steps if you want to deepen your skills and Knowledge\n",
    "\n",
    "By the end, you'll understand the **core steps** in building an ML model and how it applies to problems like **finding matches in dating apps**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd88ca6f953a25e",
   "metadata": {},
   "source": [
    "# 1. Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45ba5-25a9-4b50-941a-32a0e587ac50",
   "metadata": {},
   "source": [
    "## Types of Machine Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898c7bb-c5ca-4487-be03-9869b8ac82d4",
   "metadata": {},
   "source": [
    "Machine learning is broadly categorized into three types:  \n",
    "\n",
    "- **Supervised Learning** ‚Äì Learning from labeled examples (e.g., spam detection, image classification).  \n",
    "- **Unsupervised Learning** ‚Äì Finding patterns in **unlabeled** data (e.g., clustering, anomaly detection).  \n",
    "- **Reinforcement Learning** ‚Äì Learning by interacting with an environment (e.g., game-playing agents, robotics).  \n",
    "\n",
    "Today, we will **focus solely on supervised learning**, the most widely used ML approach in industry.\n",
    "\n",
    "Whenever we say ML we mean Supervised and vice versa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0db3a4-aeb6-4605-9035-5e9f0b4185b7",
   "metadata": {},
   "source": [
    "## When Do We Use Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156d4f-16b9-4cf8-be62-39f871735dec",
   "metadata": {},
   "source": [
    "### Supervised learning ~ Test driven developement\n",
    "\n",
    "Imagine you need to write a function, but instead of defining its logic, you only have a **set of test cases**.\n",
    "\n",
    "Machine Learning is **like solving a problem by passing tests, without writing explicit rules**:\n",
    "\n",
    "‚úÖ A pattern exists.  \n",
    "‚ùå But we don‚Äôt know how to define it with hardcoded logic.  \n",
    "‚úÖ We have examples (data) showing expected results (Tests).\n",
    "\n",
    "Instead of manually coding the transformation logic, we **show examples, an a model \"learns\" from it**‚Äîjust like refining an implementation until all tests pass.\n",
    "\n",
    "![Machine Learning Types](data/presentation_files/show_you_the_door.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1c5e3-ee15-4989-b637-27b1d9a01375",
   "metadata": {},
   "source": [
    "## Core Components of an ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf53a2d-ae25-4508-ae0b-a66e456c7696",
   "metadata": {},
   "source": [
    "### Defining the Problem  \n",
    "In machine learning, we aim to approximate an _UNKNOWN_ **Target function**:\n",
    "\n",
    "$$f: X \\rightarrow Y$$\n",
    "\n",
    "Where:\n",
    "- **X** is the input space (features, vectore, images, text,....).\n",
    "- **Y** is the output space (labels or predictions: {0,1}, [-1,1], {cats,dogs}, {documents_labels}).\n",
    "- The goal is to learn a function **f** that best maps inputs to outputs.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Dataset D (Examples, Test Cases)  \n",
    "Our training data consists of **labeled examples**:\n",
    "\n",
    "$$(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$$\n",
    "\n",
    "- Each **x** is an input (e.g., an image, text, or structured data).\n",
    "- Each **y** is the correct output (e.g., a category label).\n",
    "- The dataset acts like a **test suite** for learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Hypothesis Class (Possible Implementations H)  \n",
    "- The **hypothesis class** defines the set of functions the model can learn.  \n",
    "- This may help focus our algorithm in practice. In theory it doesn't limit the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Learning Algorithm A (Process of Finding f)  \n",
    "- The **learning algorithm** _searches for the best_ function **h ‚àà H**, where **H** is the **hypothesis set**.\n",
    "- _searches for the best_ = **optimizes parameters** to minimize errors. This Process is also called **expectancy loss minimazation**.\n",
    "  $$ \n",
    "\\min_{h \\in H} \\sum_{i=1}^{N} L(h(x_i), y_i)\n",
    "$$\n",
    "- Think of this as an **automated debugging and optimization** process‚Äîlike refining an implementation until all test cases pass.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"data/presentation_files/learning_paradigm.png\" alt=\"ML Components\" width=\"600\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "- Different hypothesis sets (e.g., linear regression, decision trees, neural networks) provides Different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b51430-3ca4-4aaa-a65e-50c11cc17be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Enter an example on linear regresion\n",
    "# (x1,y1), .... ()\n",
    "# H = aX+b -> infinite set\n",
    "# Learning algorithm = find a and b that explains the dataset best \n",
    "\n",
    "# h1\n",
    "# h2\n",
    "# h10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6df26f-8aa4-4aac-86a3-24120bac05a6",
   "metadata": {},
   "source": [
    "# 2. A Data Science Project - Our own Tinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecdd66-3681-4a2f-953f-1bc2713439d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Starts with the Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3229de-f004-49a7-bec6-0c7407c4e1f0",
   "metadata": {},
   "source": [
    "### Key Questions to Ask:  \n",
    "üîπ What are we trying to solve?  \n",
    "üîπ Why is ML a good solution for this?  \n",
    "üîπ What data do we have (or need to collect)?  \n",
    "\n",
    "Our goal is not to **force ML** but to **determine whether ML is the right approach**.\n",
    "\n",
    "\n",
    "#### Example when ML is not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909fb68-b542-4e8a-a74f-d7cd656e54fb",
   "metadata": {},
   "source": [
    "### Build your own Tinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee511e-8b3b-4023-b27c-e0c6e4e1fc1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We want to build a model that **learns what you find attractive**.  \n",
    "\n",
    "#### Why is this a Good ML Problem?  \n",
    "‚úÖ **There is a pattern** ‚Äì Your preferences are not random.  \n",
    "‚ùå **It‚Äôs hard to code manually** ‚Äì You can‚Äôt write explicit rules for what makes someone attractive.  \n",
    "‚úÖ **It‚Äôs easy to show with examples** ‚Äì Instead of defining a rule, you can **label examples** of what you like.  \n",
    "\n",
    "This makes it a **classic supervised learning problem**:  \n",
    "- **Inputs ($X$)**: Images of people.  \n",
    "- **Outputs ($Y$)**: Your rating (like/dislike).  \n",
    "- **Goal**: Learn a function $f: X \\to Y$ that predicts your taste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4be1e-acc2-4dfe-b3cf-da41f54b9c4c",
   "metadata": {},
   "source": [
    "To build our model, we must:  \n",
    "\n",
    "### 1Ô∏è‚É£ **Create a Dataset**  \n",
    "Choosing data can come from two directions:  \n",
    "1. **Use existing data** ‚Äì Work with what you already have.  \n",
    "   - Extract relevant features from it.  \n",
    "2. **Generate new data** ‚Äì Collect data based on your understanding of the problem.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Choose a Hypothesis Class**  \n",
    "The hypothesis class defines the set of functions the model can learn. Common choices include:  \n",
    "\n",
    "- **Decision Trees** ‚Äì Learn a series of if-else rules to classify inputs. Needs well defined features (e.g. height, weight, skin_color, eye_color, hair_type, ...)\n",
    "- **Linear Regression** ‚Äì Model relationships between features using a weighted sum.  Needs well defined numeric, *continoues, features.\n",
    "- **Convolutional Neural Networks (CNNs)** ‚Äì Extract spatial patterns from images, making them ideal for vision tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Our Case: Images + CNN**  \n",
    "In our case, we will use **images** because:  \n",
    "- They are the most natural way to represent visual preferences.  \n",
    "- They allow us to capture complex patterns that are hard to define manually.  \n",
    "\n",
    "Since CNNs excel at **image-based learning**, we will use a **Convolutional Neural Network (CNN)** to model preferences.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Select a Learning Algorithm**  \n",
    "Once we choose a hypothesis class, we need an algorithm to **train** the model.  \n",
    "\n",
    "- In **99.999...% of cases**, the model you work with has a **built-in learning algorithm**.  \n",
    "- For **Neural Networks**, the standard training method is **Backpropagation**.  \n",
    "\n",
    "(*We will dive a little bit in on how **Backpropagation** works in the training section.*)\n",
    "\n",
    "### 5Ô∏è‚É£ **Evaluate results**\n",
    "improve this bullet\n",
    "\n",
    "\n",
    "Create a detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf5a54-c086-435a-9c9e-5af83db22547",
   "metadata": {},
   "source": [
    "# 3. Creating Our Dataset - The most important part of them all. Allways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b070e-48a2-4538-9a96-fb48ecc5af1e",
   "metadata": {},
   "source": [
    "### Why Does Data Matter? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfd763-c7dc-4080-b47c-45cb273f4ebe",
   "metadata": {},
   "source": [
    " \n",
    "Before training a model, we need **high-quality data**.  \n",
    "\n",
    "üîπ **You Can‚Äôt Optimize Without Data** ‚Äì In ML, we have **three main components**:  \n",
    "   - **Data** ‚Äì The foundation; without it, learning is impossible.  \n",
    "   - **Hypothesis Class** ‚Äì The set of possible functions the model can learn.  \n",
    "   - **Learning Algorithm** ‚Äì The method to optimize parameters.  \n",
    "\n",
    "If we **remove the learning algorithm**, we can still train a model manually.  \n",
    "If we **choose a suboptimal hypothesis class**, we still learn something.  \n",
    "But **without data, nothing works.**  \n",
    "\n",
    "üîπ **Garbage In, Garbage Out** ‚Äì A model is only as good as the data it learns from.  \n",
    "\n",
    "---\n",
    "\n",
    "## Our Task: Collect Personal Preference Data  \n",
    "To train our model, we need labeled examples of **what we find attractive**.  \n",
    "\n",
    "We will use the **Photo-Rater App** to label images, creating a dataset that reflects individual preferences.  \n",
    "\n",
    "üîú Next: Let's start label data!  \n",
    "\n",
    "![Tagg it all](data/presentation_files/xally.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d5db4-50de-443d-8112-c8a4080df425",
   "metadata": {},
   "source": [
    "Go now... swipe right and left, and come back when you're done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286df2c-6f94-480b-8b79-a42660b98bf1",
   "metadata": {},
   "source": [
    "# 4. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d11dc-9f91-4dfc-aff7-939b5b56ea71",
   "metadata": {},
   "source": [
    "## 4.1 Prepare Our Dataset for Training  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2f633-ab0d-4704-b10f-96ce3402ca3c",
   "metadata": {},
   "source": [
    "Now that we have labeled data, we need to **organize it for training**.  \n",
    "\n",
    "### 1Ô∏è‚É£ Train, Validation, and Test Split  \n",
    "To evaluate our model properly, we split the data into three parts:  \n",
    "\n",
    "- **Training Set** ‚Äì Used to train the model.  \n",
    "- **Validation Set** ‚Äì Used to tune hyperparameters and detect overfitting.  \n",
    "- **Test Set** ‚Äì Used to evaluate final model performance on unseen data.  \n",
    "\n",
    "Finally - some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e1af59-4361-4146-886a-a6751de80ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete! üéâ\n",
      "Train: 139 (Like: 23, Dislike: 116)\n",
      "Validation: 30 (Like: 5, Dislike: 25)\n",
      "Test: 32 (Like: 6, Dislike: 26)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Define dataset paths\n",
    "DATASET_DIR = \"data/dataset\"  # Base dataset directory\n",
    "OUTPUT_DIR = \"data\"  # Where train/val/test splits will be stored\n",
    "\n",
    "# Define train, validation, and test split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Ensure the output directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for label in [\"Like\", \"Dislike\"]:  # Ensure we maintain class labels\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, split, label), exist_ok=True)\n",
    "\n",
    "# Function to split and copy images while maintaining original dataset\n",
    "def split_and_copy(label):\n",
    "    label_dir = Path(DATASET_DIR) / label\n",
    "    all_images = list(label_dir.glob(\"*.jpg\"))  # Adjust for different image formats if needed\n",
    "    random.shuffle(all_images)  # Shuffle with fixed seed for reproducibility\n",
    "\n",
    "    # Compute split sizes\n",
    "    num_images = len(all_images)\n",
    "    train_split = int(num_images * TRAIN_RATIO)\n",
    "    val_split = int(num_images * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "    # Split dataset\n",
    "    train_files = all_images[:train_split]\n",
    "    val_files = all_images[train_split:val_split]\n",
    "    test_files = all_images[val_split:]\n",
    "\n",
    "    # Function to copy files instead of moving\n",
    "    def copy_files(files, split):\n",
    "        dest_dir = Path(OUTPUT_DIR) / split / label\n",
    "        existing_files = set(f.name for f in dest_dir.glob(\"*.jpg\"))  # Track existing files\n",
    "        for file in files:\n",
    "            if file.name not in existing_files:  # Avoid duplicates if rerunning\n",
    "                shutil.copy(str(file), os.path.join(dest_dir, file.name))\n",
    "\n",
    "    # Copy files to respective folders\n",
    "    copy_files(train_files, \"train\")\n",
    "    copy_files(val_files, \"val\")\n",
    "    copy_files(test_files, \"test\")\n",
    "\n",
    "    return len(train_files), len(val_files), len(test_files)\n",
    "\n",
    "# Process both classes\n",
    "train_like, val_like, test_like = split_and_copy(\"Like\")\n",
    "train_dislike, val_dislike, test_dislike = split_and_copy(\"Dislike\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Dataset split complete! üéâ\")\n",
    "print(f\"Train: {train_like + train_dislike} (Like: {train_like}, Dislike: {train_dislike})\")\n",
    "print(f\"Validation: {val_like + val_dislike} (Like: {val_like}, Dislike: {val_dislike})\")\n",
    "print(f\"Test: {test_like + test_dislike} (Like: {test_like}, Dislike: {test_dislike})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f502bfb8-59a7-4e25-8d11-cebe64e402ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Like: 23 images\n",
      "Train - Dislike: 116 images\n",
      "Val - Like: 5 images\n",
      "Val - Dislike: 25 images\n",
      "Test - Like: 6 images\n",
      "Test - Dislike: 26 images\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Count files in each split\n",
    "def count_files():\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for label in [\"Like\", \"Dislike\"]:\n",
    "            path = Path(OUTPUT_DIR) / split / label\n",
    "            num_files = len(list(path.glob(\"*.jpg\")))\n",
    "            print(f\"{split.capitalize()} - {label}: {num_files} images\")\n",
    "\n",
    "count_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f5f93-165d-4e03-b16a-70c9bfeec7b0",
   "metadata": {},
   "source": [
    "### 4.1.1 Why are we doing this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e70f5-7ee0-40a3-886b-34eafa339815",
   "metadata": {},
   "source": [
    "### 4.1.2 Overfitting vs. Generalization ‚Äì A TDD Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29c6cc-0039-4512-8e84-ba4f4166e40b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "üöÄ Imagine You‚Äôre an Engineer...\n",
    "I give you **100 test cases** and tell you:\n",
    "\n",
    "üëâ **\"Just make sure all these tests pass.\"**\n",
    "\n",
    "A straighforward solution would be:\n",
    "look at each $test_i$ that has $input_i$ and $expected_output_i$\n",
    "```python\n",
    " def my_function(some_input):\n",
    "     if some_input==input_i:  \n",
    "         return expected_output_i  \n",
    "     else:  \n",
    "         return None  \n",
    "```\n",
    "This **hardcodes answers** instead of solving the real problem.  \n",
    "‚úÖ **Passes all known tests.**  \n",
    "‚ùå **Fails on new cases.**  \n",
    "**This is overfitting!** In extreme cases, it's just **memorization**.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **How Do We Ensure Generalization?**\n",
    "Because I'm smart, I **don't give you all 100 test cases**!  \n",
    "üëâ **I give you 80**, but keep **20 hidden.**  \n",
    "\n",
    "I tell you:  \n",
    "**\"Write a good function based on these 80 examples.  \n",
    "If it also works on my secret 20 tests, I‚Äôll give you 500 shekels in BuyMe!\"**  \n",
    "\n",
    "##### **Two advantages for this approch?**\n",
    "‚úÖ **Now you must generalize!**  You can't just memorize, you have to find the real pattern!  \n",
    "‚úÖ **I can test your generalization** Because I kep 20 examples to myself.\n",
    " \n",
    "This is **exactly why we split our dataset** into:  \n",
    "- **Train Set (80%)** ‚Üí The model learns from this.  \n",
    "- **Test Set (20%)** ‚Üí The model must perform well on unseen data.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **Key Takeaway:**  \n",
    "üí° A model that **only memorizes the training set is useless**. We need **generalization** for real-world performance!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3be88-cd18-4db1-9c85-870e96504a45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1.3 Good Questions at This Point ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758d9dd-8aa1-40e2-bb35-1c5719225400",
   "metadata": {},
   "source": [
    "\n",
    "1Ô∏è‚É£ **How do we evaluate this performance?**  \n",
    "üì¢ We will talk about it in the **Evaluation section** (after training).  \n",
    "\n",
    "2Ô∏è‚É£ **This explanation doesn't explain how the model knows not to overfit the 80 examples I gave him.**  \n",
    "That is very true! **Splitting the data only allows us to measure generalization performance.**  \n",
    "Just like we gave the engineer a **500 shekels motivation**, we need ways to **motivate the model not to overfit**.  \n",
    "\n",
    "This is where **Regularization** comes in! Regularization techniques **penalize complexity** to encourage the model to find simpler, more generalizable patterns. This is advanced so we might just mention this during the follwoing training session! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1ecd6-577e-4019-9940-062fa25f1f2f",
   "metadata": {},
   "source": [
    "### 4.1.4 A Few words on Preprocessing Before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7836d28-54d5-433d-b2a9-c687983d13be",
   "metadata": {},
   "source": [
    "In real-world cases, we usually preprocess the data further before passing it to a model. **Preprocessing** can involve:  \n",
    "\n",
    "üîπ **Feature Extraction** - Creating additional features from raw data. While this is **less common in images** due to the nature of CNNs, it is **very useful in other models**.\n",
    "* Example (House Prices) ‚Üí Instead of using the raw address, we can preprocess it into ‚Äúdistance from the city center‚Äù, turning an informative but hard-to-use string into a continuous, easy-to-work-with number.\n",
    "* Example (NLP) ‚Üí Before using text in a model, we must convert words into numbers, and better to meaningful numerical vectors (a.k.a. embeddings) to capture their relationships and meanings.\n",
    "  \n",
    "üîπ **Data Manipulation** ‚Äì Standardizing input formats (e.g., resizing images, filtering out low-resolution images, handling missing values).  \n",
    "üîπ **Normalization & Scaling** ‚Äì Ensuring that numerical features are on a similar scale to improve training stability.  \n",
    "\n",
    "##### üöÄ Why Is Preprocessing Crucial?  \n",
    "Preprocessing is usually where a Data Scientist has the most **room to shine**! Unlike modeling, where architectures and optimizers are often well-defined, **there is no single \"correct\" way to preprocess data**.  \n",
    "**_\"It is art\"_** as some Feinshmekers would say  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffc5dc-4232-41e1-a23c-20285be7c0a4",
   "metadata": {},
   "source": [
    "## 4.2 (Really) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf032053-fc1b-4e95-b2a2-6bc8d6f99ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Train samples: 139\n",
      "Validation samples: 30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU\n",
    "# The device is usuall cuda, but on mac we have a locall \"mps\" as GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"data/train\"\n",
    "val_dir = \"data/val\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to match MobileNet input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849826cf-59ac-41f0-9263-7fe949bfde38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hananzaichyk/Library/Caches/pypoetry/virtualenvs/sentra-anton-AD5miZ6v-py3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hananzaichyk/Library/Caches/pypoetry/virtualenvs/sentra-anton-AD5miZ6v-py3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained MobileNet model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the classifier for our binary classification task (Like vs. Dislike)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 2)  # Two output classes (Like & Dislike)\n",
    ")\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9ebaf-5da4-4076-9cf1-ef7280213bd3",
   "metadata": {},
   "source": [
    "**Assigning a Loss Functions**\n",
    "\n",
    "A **loss function** measures how wrong our model's predictions are.\n",
    "**Cross-Entropy** (Log Loss) function works well for **binary classification** (Like vs. Dislike), \n",
    "\\[\n",
    "L = - \\sum y \\log(\\hat{y})\n",
    "\\]\n",
    "‚úÖ Encourages the model to assign **high probability to the correct class**.  \n",
    "‚úÖ Punishes confident wrong predictions **more than uncertain ones**.  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc94913-e613-4076-8130-db981fd95db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd02fb9-8912-467e-9e94-d21e039d0eb0",
   "metadata": {},
   "source": [
    "**Explaining the Training Loop: Gradient Descent & Backpropagation**\n",
    "\n",
    "**Gradient Descent ‚Äì The Optimization Process**\n",
    "Gradient Descent guides how we update the model‚Äôs weights to reduce loss.  \n",
    "\n",
    "\\[\n",
    "\\theta := \\theta - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}\n",
    "\\]\n",
    "\n",
    "üìå **What‚Äôs happening?**  \n",
    "- \\( \\theta \\) = Model parameters (weights & biases).  \n",
    "- \\( \\eta \\) = Learning rate (step size).  \n",
    "- \\( \\frac{\\partial L}{\\partial \\theta} \\) = Gradient of the loss function w.r.t. \\( \\theta \\).  \n",
    "\n",
    "Each step **adjusts the weights** in the direction that lowers the loss.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How Does Training Work? (Gradient Descent in Action)\n",
    "\n",
    "It consists of three key steps:\n",
    "\n",
    "1Ô∏è‚É£ **Forward Pass** ‚Üí The model processes input images and makes predictions.  \n",
    "2Ô∏è‚É£ **Compute Loss** ‚Üí Compare predictions to the true labels using the loss function.  \n",
    "3Ô∏è‚É£ **Backward Pass (Backpropagation)** ‚Üí Compute how much each weight contributed to the error and adjust weights iteratively, layer by layer. It propagates the loss function **backward through the network**, layer by layer, using the chain rule of derivatives, and updates weights based on their gradients.\n",
    "\n",
    "(Add sketch here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1815ea93-0d04-4c83-ad0c-6ac715ae7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6140\n",
      "Epoch [2/5], Loss: 0.2759\n",
      "Epoch [3/5], Loss: 0.1210\n",
      "Epoch [4/5], Loss: 0.0743\n",
      "Epoch [5/5], Loss: 0.0858\n",
      "Training complete! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer to be Adam (advance GD) with a learning rate of 0.001 \n",
    "# lr=0.001 is a common practice in the industry.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Keep training short for the workshop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete! üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77beea8c-1c3d-4f22-bed5-c228f1ac70e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b4209-8d34-4f5f-a29f-fd0c6b5cbbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8918e3-1926-4256-b39f-df104872f6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ()",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
