{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1f6e0a96ffe651",
   "metadata": {},
   "source": [
    "# Welcome to the ML-Entry Workshop! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049c57f-63b2-43ab-8977-864ebfb548a9",
   "metadata": {},
   "source": [
    "## What We‚Äôll Cover:\n",
    "- Introduction\n",
    "- How does a Data Science project look?\n",
    "- How do we choose a model and train it?\n",
    "- Hands-on experience: Building Tinder! Create a dataset, and train a model to solve a real-world problem.\n",
    "- Theory Keyconcepts: Hypothesis set, Train vs Validation/Test, loss function, Backpropagtion, CNN...\n",
    "- Take it further: recommended steps if you want to deepen your skills and Knowledge\n",
    "\n",
    "By the end, you'll understand the **core steps** in building an ML model and how it applies to problems like **finding matches in dating apps**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a0c1925c5eb46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Basic preps:\n",
    "1. Install uv: `brew install uv`\n",
    "2. clone the workshop repo: `git clone git@github.com:zaichyk/ml-entry-workshop.git`\n",
    "3. Stop everything and order Lunch to 12:30\n",
    "4. Go to repo root and run : `uv sync`\n",
    "5. Activate your venv `source .venv/bin/activate` (you are now in your uv venv)\n",
    "6. Create a new ipython kernel `python -m ipykernel install --user --name=mlworkshop --display-name \"Python (ML Workshop)\"`\n",
    "7. in the top right corner choose your new kernel \"Python (ML Workshop)\"\n",
    "8. cd `ml_entry_workshop` open `workshop_notebook.ipynb`\n",
    "9. If you tagged men - remove images 21,37 and 243 from your data! (for some reason they model can't read them)\n",
    "10. You are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd88ca6f953a25e",
   "metadata": {},
   "source": [
    "# 1. Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfba5f-0bcb-4b2d-b6f7-374eb29722f9",
   "metadata": {},
   "source": [
    "### 1.0 About me\n",
    "*‚ÄúHi! I‚Äôm Hanan, 36, Father to Ofek, Husband of Inbal,<br>And I like explaining complex things in simple terms* üòä *‚Äù*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45ba5-25a9-4b50-941a-32a0e587ac50",
   "metadata": {},
   "source": [
    "## 1.1 Types of Machine Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898c7bb-c5ca-4487-be03-9869b8ac82d4",
   "metadata": {},
   "source": [
    "Machine learning is broadly categorized into three types:  \n",
    "\n",
    "- **Supervised Learning** ‚Äì Learning from labeled examples (e.g., spam detection, image classification).  \n",
    "- **Unsupervised Learning** ‚Äì Finding patterns in **unlabeled** data (e.g., clustering, anomaly detection).  \n",
    "- **Reinforcement Learning** ‚Äì Learning by interacting with an environment (e.g., game-playing agents, robotics).  \n",
    "\n",
    "Today, we will **focus solely on supervised learning**, the most widely used ML approach in industry.\n",
    "\n",
    "Whenever we say ML we mean Supervised and vice versa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0db3a4-aeb6-4605-9035-5e9f0b4185b7",
   "metadata": {},
   "source": [
    "## 1.2 When Do We Use Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81156d4f-16b9-4cf8-be62-39f871735dec",
   "metadata": {},
   "source": [
    "#### 3 simple conditions:\n",
    "‚úÖ We believe a pattern exists.  \n",
    "‚ùå We don‚Äôt know how to define it with hardcoded logic.  \n",
    "‚úÖ We have examples (data) showing expected results (Tests).\n",
    "\n",
    "**Supervised learning ~ Test driven developement**\n",
    "Test-Driven Development (TDD) is a software development approach where tests are written before the actual code, guiding implementation and ensuring functionality through iterative cycles of failing tests, coding, and refactoring.\n",
    "\n",
    "Machine (supervisied) Learning is **like writing tests, and let someone else write the logic, as long as they pass the tests**:\n",
    "\n",
    "Instead of manually coding logic, we **show examples, an a model \"learns\" what should be done**‚Äîjust like refining an implementation until all tests pass.\n",
    "\n",
    "![Machine Learning Types](data/presentation_files/show_you_the_door.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1c5e3-ee15-4989-b637-27b1d9a01375",
   "metadata": {},
   "source": [
    "## 1.3 Core Components of an ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67750b10-b4a6-49c6-a4d7-bf92fa46c525",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "In machine learning, we aim to approximate an **UNKNOWN** target function:\n",
    "\n",
    "$$f: X \\rightarrow Y$$\n",
    "\n",
    "Where:\n",
    "- **X** is the input space (features, vector, images, text,....).\n",
    "- **Y** is the output space (labels or predictions: {0,1}, [-1,1], {cats,dogs}, {documents_labels}).\n",
    "- The goal is to learn a function **f** that best maps inputs to outputs.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1Ô∏è‚É£ Dataset D (Examples, Test Cases)  \n",
    "Our training data consists of **labeled examples**:\n",
    "\n",
    "$$(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$$\n",
    "\n",
    "- Each **x** is an input (e.g., an image, text, or structured data).\n",
    "- Each **y** is the correct output (e.g., a category label).\n",
    "- The dataset is our sole source of information about the unknown target function\n",
    "- We assume the examples are *independent and identically distributed* (i.i.d) ‚Äî we state this for correctness won't elaborate further.\n",
    "* Coming back to the TDD analogy, the dataset serves as a set of test cases that guide the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cd2e8-fd52-4fa0-adbf-3576d61fca54",
   "metadata": {},
   "source": [
    "**Good question at this point:**\n",
    "\n",
    "Is the model deterministic?\n",
    "\t‚Ä¢\tYes, ML models are deterministic ‚Üí For the same input X, you get the same output Y.\n",
    "\t‚Ä¢\tBut what about ChatGPT? It doesn‚Äôt always generate the same response.\n",
    "\n",
    "Explanation:\n",
    "\t‚Ä¢\tIt depends on how you define the ML model vs. post-processing.\n",
    "\t‚Ä¢\tLLMs vs. Classification Models\n",
    "\t‚Ä¢\tClassification models (e.g., fraud detection) directly map X to Y, producing deterministic results.\n",
    "\t‚Ä¢\tLLMs are generative: They include a probability mechanism for text generation.\n",
    "\t‚Ä¢\tCore model of an LLM is deterministic:\n",
    "\t‚Ä¢\tInput X (text) ‚Üí Output Y (logits, a probability distribution over words).\n",
    "\t‚Ä¢\tFor the same X, you get the same distribution vector.\n",
    "\t‚Ä¢\tRandomness comes from the decoding process (e.g., sampling, temperature settings), which chooses words from the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb69381-6ec4-451e-bc0e-84a6df36365d",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2Ô∏è‚É£ Hypothesis Class (Possible Implementations H)  \n",
    "- The **hypothesis class** defines the set of functions the model can learn.\n",
    "- This helps focus our algorithm.\n",
    "- In theory it doesn't even limit the model, as we can set $H=\\{set\\ of\\ all\\ functions\\}$\n",
    "- In practice We never mention a Hypothesis set, but every model you know rely on this definition.\n",
    "- In theory this doesn't even limit the model, as we can set $H=\\{set\\ of\\ all\\ functions\\}$\n",
    "- In practice We never mention a Hypothesis set, but every model you know rely on this definition in it's training/optimisation phase.\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Learning Algorithm A (Process of Finding f)  \n",
    "- The **learning algorithm** _searches for the best_ function **h ‚àà H**, where **H** is the **hypothesis set**.\n",
    "- _searches for the best_ = **optimizes parameters** to minimize errors. This Process is also called **expectancy loss minimazation**.\n",
    "  $$ \n",
    "\\min_{h \\in H} \\sum_{i=1}^{N} L(h(x_i), y_i)\\ \\ \\ \\ (\\ for\\ example\\  \\min_{h \\in H} \\sum_{i=1}^{N} (h(x_i) - y_i)^2)\n",
    "$$\n",
    "- Think of this as an **automated debugging and optimization** process‚Äîlike refining an implementation until all test cases pass.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"data/presentation_files/learning_paradigm.png\" alt=\"ML Components\" width=\"600\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb6735-7cbd-4123-8ace-36a21643182a",
   "metadata": {},
   "source": [
    "- **Examples:**\n",
    "    - **Linear Regression**:\n",
    "        - \\( H \\) = Set of linear functions of the form \\( h(x) = a \\cdot x + b \\).\n",
    "        - \\( L \\) = Learn the best **\\( a \\)** and **\\( b \\)** that explain the dataset.\n",
    "\n",
    "    - **Decision Trees**:\n",
    "        - \\( H \\) = Set of **tree-structured functions**, where each node represents a feature split.\n",
    "            - Example: Deciding whether to go outside based on weather conditions (**Sunny, Humid, Temperature**).\n",
    "            - Data: Past experience compared to historical weather.\n",
    "        - \\( L \\) = Learn the best splits for **Sunny, Humid, Temperature**.\n",
    "\n",
    "    - **Neural Networks**:\n",
    "        - \\( H \\) = A structured set of **parameters (weights)** and **mathematical operations** in a predefined order (architecture).\n",
    "        - \\( L \\) = Learn the weights that best fit the data, typically using some form of **SGD** (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9890fc5-0531-4d7d-8c02-4b30f3c93ce6",
   "metadata": {},
   "source": [
    "## 1.4 Data is Essential, Model Choice is Secondary\n",
    "We defined three key components in the learning paradigm: Data, Hypothesis Set, and Learning Algorithm.\n",
    "Among these, data is the most crucial starting point‚Äîwithout high-quality, representative data, even the best hypothesis class and learning algorithm will fail.\n",
    "\n",
    "**Why is data so important?**\n",
    "- The dataset is our only information about the unknown target function  f .\n",
    "- **Garbage in, garbage out**:\n",
    "    - Poor or biased data leads to poor generalization, regardless of model complexity.\n",
    "    - Lack of data is leading to overfitting\n",
    "- A simple model trained on high-quality data often outperforms a complex model trained on noisy or unrepresentative data.\n",
    "\n",
    "**Creating high Quality dataset is hard**\n",
    "- The dataset contains all the infromation we don't know to formalize.\n",
    "- It takes time to find the right examples.\n",
    "- It takes time or money to tag.\n",
    "- It is usually boring.\n",
    "- It sometimes takes time to understand what examples I need to show the model.\n",
    "- By creating a good dataset you actually understand the problem better and better.\n",
    "\n",
    "\n",
    "**‚ö† The Most Important Takeaway of This Entire Workshop ‚ö†**\n",
    "\n",
    "- The first focus in any ML project must be collecting, cleaning, and understanding the data before refining models.\n",
    "- No model can fix bad or missing data. This is supported both in theory and in practice\n",
    "- Most effective way to improve a model - find the bad examples in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6df26f-8aa4-4aac-86a3-24120bac05a6",
   "metadata": {},
   "source": [
    "# 2. A Data Science Project - Our own Tinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3229de-f004-49a7-bec6-0c7407c4e1f0",
   "metadata": {},
   "source": [
    "### Key Questions to Ask In Every Project:  \n",
    "üîπ What are we trying to solve?  \n",
    "üîπ Is ML a good solution for this?  \n",
    "üîπ What data do we have (or need to collect)?  \n",
    "\n",
    "Our goal is not to **force ML** but to **determine whether ML is the right approach**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df41dac-15ef-4277-a3b3-747a7b60625d",
   "metadata": {},
   "source": [
    "**Negative examples: When ML is not the way to go**\n",
    "\n",
    "1. **Identifying Country Names in a Text**\n",
    "    - Tempting ML Approach: Use an NLP model to detect mentions of countries in text.\n",
    "    - Better Alternative: A simple lookup table or regular expressions with 195 country names:\n",
    "        - Can achieve 99% accuracy without ML.\n",
    "        - Faster and simpler both in development and in running time\n",
    "    - ML would introduce unnecessary complexity.\n",
    "    - ML is a good option if you detect 20 categories at a time, and one of them is country name.<br>\n",
    "<br>\n",
    "2. **Categorizing Financial Documents**\n",
    "    - Forms like W-2, 1040, often begin with a fixed phrase or code that can easily be matched with a **regex** (e.g., ‚ÄúForm W-2 Wage and Tax Statement‚Äù).\n",
    "    - ML adds unnecessary overhead (e.g., collecting labeled training data, handling OCR variations).\n",
    "    - ML is the way to go if you want to categorize dozens of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909fb68-b542-4e8a-a74f-d7cd656e54fb",
   "metadata": {},
   "source": [
    "### Build your own Tinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee511e-8b3b-4023-b27c-e0c6e4e1fc1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We want to build a model that **learns what you find attractive**.  \n",
    "\n",
    "#### Why is this a Good ML Problem?  \n",
    "‚úÖ **There is a pattern** ‚Äì Your preferences are not random.  \n",
    "‚ùå **It‚Äôs hard to code manually** ‚Äì You can‚Äôt write explicit rules for what makes someone attractive.  \n",
    "‚úÖ **It‚Äôs easy to show with examples** ‚Äì Instead of defining a rule, you can **label examples** of what you like.  \n",
    "\n",
    "This makes it a **classic supervised learning problem**:  \n",
    "- **Inputs ($X$)**: Images of people.  \n",
    "- **Outputs ($Y$)**: Your rating (Like/Dislike).  \n",
    "- **Goal**: Learn a function $f: X \\to Y$ that predicts your taste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4be1e-acc2-4dfe-b3cf-da41f54b9c4c",
   "metadata": {},
   "source": [
    "To build our model, we must:  \n",
    "\n",
    "### 1Ô∏è‚É£ **Create a Dataset**  \n",
    "Choosing data can come from two directions:  \n",
    "1. **Use existing data** ‚Äì Work with what you already have.  \n",
    "   - Extract relevant features from it.  \n",
    "2. **Generate new data** ‚Äì Collect data based on your understanding of the problem.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Choose a Hypothesis Class**  \n",
    "The hypothesis class defines the set of functions the model can learn. Common choices include:  \n",
    "\n",
    "- **Decision Trees** ‚Äì Learn a series of if-else rules to classify inputs. Needs well defined features (e.g. height, weight, skin_color, eye_color, hair_type, ...)\n",
    "- **Linear Regression** ‚Äì Model relationships between features using a weighted sum.  Needs well defined numeric, *continoues, features.\n",
    "- **Convolutional Neural Networks (CNNs)** ‚Äì Extract spatial patterns from images, making them ideal for vision tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Our Case: Images + CNN**  \n",
    "In our case, we will use **images** because:  \n",
    "- They are the most natural way to represent visual preferences.  \n",
    "- They allow us to capture complex patterns that are hard to define manually.  \n",
    "\n",
    "Since CNNs excel at **image-based learning**, we will use a **Convolutional Neural Network (CNN)** to model preferences.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Select a Learning Algorithm**  \n",
    "Once we choose a hypothesis class, we need an algorithm to **train** the model.  \n",
    "\n",
    "- In **99.999...% of cases**, the model you work with has a **built-in learning algorithm**.  \n",
    "- For **Neural Networks**, the standard training method is **Backpropagation**.  \n",
    "\n",
    "(*We will dive a little bit in on how **Backpropagation** works in the training section.*)\n",
    "\n",
    "### 5Ô∏è‚É£ **Evaluate results**\n",
    "improve this bullet\n",
    "\n",
    "\n",
    "Create a detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf5a54-c086-435a-9c9e-5af83db22547",
   "metadata": {},
   "source": [
    "# 3. Creating Our Dataset - The most important part of them all. Allways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b070e-48a2-4538-9a96-fb48ecc5af1e",
   "metadata": {},
   "source": [
    "**Reminder: Why Does Data Matter?**\n",
    "Before training a model, we need **high-quality data**.  \n",
    "\n",
    "üîπ **You Can‚Äôt Optimize Without Data** ‚Äì In ML, we have **three main components**:  \n",
    "   - **Data** ‚Äì The foundation; without it, learning is impossible.  \n",
    "   - **Hypothesis Class** ‚Äì The set of possible functions the model can learn.  \n",
    "   - **Learning Algorithm** ‚Äì The method to optimize parameters.  \n",
    "\n",
    "If we **remove the learning algorithm**, we can still train a model manually.  \n",
    "If we **choose a suboptimal hypothesis class**, we still learn something.  \n",
    "But **without data, nothing works.**  \n",
    "\n",
    "üîπ **Garbage In, Garbage Out** ‚Äì A model is only as good as the data it learns from.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfd763-c7dc-4080-b47c-45cb273f4ebe",
   "metadata": {},
   "source": [
    "## 1.3.1 Create Personal Preference Data  \n",
    "To train our model, we need labeled examples of **what we find attractive**.  \n",
    "\n",
    "We will use the **Photo-Rater App** to label images, creating a dataset that reflects individual preferences.  \n",
    "\n",
    "üîú Next: Let's start label data!  \n",
    "\n",
    "![Tagg it all](data/presentation_files/xally.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d5db4-50de-443d-8112-c8a4080df425",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "1. Navigate to `ml_entry_workshop/photo_rater`\n",
    "2. Run the awesome app:`uv run photo-rater.py`\n",
    "3. Select your preference: **'Men'** or **'Women'**\n",
    "4. Start swiping-Just like Tinder:\n",
    "    - Right arrow for \"Like\" (or click \"Like\")\n",
    "    - Left arrow for \"Dislke\" (or click \"Dislike\")\n",
    "5. If you feel you had enough before you saw all images - just click 'Finish'\n",
    "6. You will be passed to a summary page where you can go over you selection and change your choices.\n",
    "7. Click save results, approve.\n",
    "8. That's it!\n",
    "\n",
    "Go now... swipe right and left, and come back when you're done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286df2c-6f94-480b-8b79-a42660b98bf1",
   "metadata": {},
   "source": [
    "# 4. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d11dc-9f91-4dfc-aff7-939b5b56ea71",
   "metadata": {},
   "source": [
    "## 4.1 Prepare Our Dataset for Training  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2f633-ab0d-4704-b10f-96ce3402ca3c",
   "metadata": {},
   "source": [
    "Now that we have labeled data, we need to **organize it for training**.  \n",
    "\n",
    "### 1Ô∏è‚É£ Train, Validation, and Test Split  \n",
    "To evaluate our model properly, we split the data into three parts:  \n",
    "\n",
    "- **Training Set** ‚Äì Used to train the model.  \n",
    "- **Validation Set** ‚Äì Used to tune hyperparameters and detect overfitting.  \n",
    "- **Test Set** ‚Äì Used to evaluate final model performance on unseen data.  \n",
    "\n",
    "Finally - some code:"
   ]
  },
  {
   "cell_type": "code",
   "id": "28e1af59-4361-4146-886a-a6751de80ac1",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Define dataset paths\n",
    "DATASET_DIR = \"data/dataset\"  # Base dataset directory\n",
    "OUTPUT_DIR = \"data/dataset_split\"  # Where train/val/test splits will be stored\n",
    "\n",
    "# Define train, validation, and test split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Ensure the output directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for label in [\"Like\", \"Dislike\"]:  # Ensure we maintain class labels\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, split, label), exist_ok=True)\n",
    "\n",
    "# Function to split and copy images while maintaining original dataset\n",
    "def split_and_copy(label):\n",
    "    label_dir = Path(DATASET_DIR) / label\n",
    "    all_images = list(label_dir.glob(\"*.jpg\"))  # Adjust for different image formats if needed\n",
    "    random.shuffle(all_images)  # Shuffle with fixed seed for reproducibility\n",
    "\n",
    "    # Compute split sizes\n",
    "    num_images = len(all_images)\n",
    "    train_split = int(num_images * TRAIN_RATIO)\n",
    "    val_split = int(num_images * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "    # Split dataset\n",
    "    train_files = all_images[:train_split]\n",
    "    val_files = all_images[train_split:val_split]\n",
    "    test_files = all_images[val_split:]\n",
    "\n",
    "    # Function to copy files instead of moving\n",
    "    def copy_files(files, split):\n",
    "        dest_dir = Path(OUTPUT_DIR) / split / label\n",
    "        existing_files = set(f.name for f in dest_dir.glob(\"*.jpg\"))  # Track existing files\n",
    "        for file in files:\n",
    "            if file.name not in existing_files:  # Avoid duplicates if rerunning\n",
    "                shutil.copy(str(file), os.path.join(dest_dir, file.name))\n",
    "\n",
    "    # Copy files to respective folders\n",
    "    copy_files(train_files, \"train\")\n",
    "    copy_files(val_files, \"val\")\n",
    "    copy_files(test_files, \"test\")\n",
    "\n",
    "    return len(train_files), len(val_files), len(test_files)\n",
    "\n",
    "# Process both classes\n",
    "train_like, val_like, test_like = split_and_copy(\"Like\")\n",
    "train_dislike, val_dislike, test_dislike = split_and_copy(\"Dislike\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Dataset split complete! üéâ\")\n",
    "print(f\"Train: {train_like + train_dislike} (Like: {train_like}, Dislike: {train_dislike})\")\n",
    "print(f\"Validation: {val_like + val_dislike} (Like: {val_like}, Dislike: {val_dislike})\")\n",
    "print(f\"Test: {test_like + test_dislike} (Like: {test_like}, Dislike: {test_dislike})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f502bfb8-59a7-4e25-8d11-cebe64e402ce",
   "metadata": {},
   "source": [
    "# Sanity check: Count files in each split\n",
    "def count_files():\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for label in [\"Like\", \"Dislike\"]:\n",
    "            path = Path(OUTPUT_DIR) / split / label\n",
    "            num_files = len(list(path.glob(\"*.jpg\")))\n",
    "            print(f\"{split.capitalize()} - {label}: {num_files} images\")\n",
    "\n",
    "count_files()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "873f5f93-165d-4e03-b16a-70c9bfeec7b0",
   "metadata": {},
   "source": [
    "### 4.1.1 Why are we doing this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e70f5-7ee0-40a3-886b-34eafa339815",
   "metadata": {},
   "source": [
    "### 4.1.2 Overfitting vs. Generalization ‚Äì A TDD Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29c6cc-0039-4512-8e84-ba4f4166e40b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "üöÄ Imagine You‚Äôre an Engineer...\n",
    "I give you **100 test cases** and tell you:\n",
    "\n",
    "üëâ **\"Just make sure all these tests pass.\"**\n",
    "\n",
    "A straighforward solution would be:\n",
    "look at each *test_i* that has *input_i* and *expected_output_i*\n",
    "```python\n",
    " def my_function(some_input):\n",
    "     if some_input==input_i:  \n",
    "         return expected_output_i  \n",
    "     else:  \n",
    "         return None  \n",
    "```\n",
    "This **hardcodes answers** instead of solving the real problem.  \n",
    "‚úÖ **Passes all known tests.**  \n",
    "‚ùå **Fails on new cases.**  \n",
    "**This is overfitting!** In extreme cases, it's just **memorization**.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **How Do We Ensure Generalization?**\n",
    "Because I'm smart, I **don't give you all 100 test cases**!  \n",
    "üëâ **I give you 80**, but keep **20 hidden.**  \n",
    "\n",
    "I tell you:  \n",
    "**\"Write a good function based on these 80 examples.  \n",
    "If it also works on my secret 20 tests, I‚Äôll give you 500 shekels in BuyMe!\"**  \n",
    "\n",
    "##### **Two advantages for this approch?**\n",
    "‚úÖ **Now you must generalize!**  You can't just memorize, you have to find the real pattern!  \n",
    "‚úÖ **I can test your generalization** Because I kep 20 examples to myself.\n",
    " \n",
    "This is **exactly why we split our dataset** into:  \n",
    "- **Train Set (80%)** ‚Üí The model learns from this.  \n",
    "- **Test Set (20%)** ‚Üí The model must perform well on unseen data.  \n",
    "\n",
    "---\n",
    "\n",
    "##### **Key Takeaway:**  \n",
    "üí° A model that **only memorizes the training set is useless**. We need **generalization** for real-world performance!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3be88-cd18-4db1-9c85-870e96504a45",
   "metadata": {},
   "source": [
    "### 4.1.3 Good Questions at This Point ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758d9dd-8aa1-40e2-bb35-1c5719225400",
   "metadata": {},
   "source": [
    "\n",
    "1Ô∏è‚É£ **How do we evaluate this performance?**  \n",
    "üì¢ We will talk about it in the **Evaluation section** (after training).  \n",
    "\n",
    "2Ô∏è‚É£ **This explanation doesn't explain how the model knows not to overfit the 80 examples I gave him.**  \n",
    "That is very true! **Splitting the data only allows us to measure generalization performance.**  \n",
    "Just like we gave the engineer a **500 shekels motivation**, we need ways to **motivate the model not to overfit**.  \n",
    "\n",
    "This is where **Regularization** comes in! Regularization techniques **penalize complexity** to encourage the model to find simpler, more generalizable patterns. This is advanced so we might just mention this during the follwoing training session! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8687e6a018018139",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data with a quadratic pattern\n",
    "n_samples = 30  # Using 30 points for clarity\n",
    "X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "y_true = X**2  # True function is quadratic (order 2)\n",
    "noise = np.random.normal(0, 1, size=n_samples) * 2\n",
    "y = y_true.ravel() + noise  # Add noise to make it realistic\n",
    "\n",
    "# Create models with different complexities\n",
    "# 1. Linear model (order 1)\n",
    "model_linear = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
    "# 2. Quadratic model (order 2) - should be a good fit\n",
    "model_quadratic = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "# 3. High-order polynomial (order 30) - will perfectly overfit\n",
    "model_complex = make_pipeline(PolynomialFeatures(30), LinearRegression())\n",
    "\n",
    "# Fit models\n",
    "model_linear.fit(X, y)\n",
    "model_quadratic.fit(X, y)\n",
    "model_complex.fit(X, y)\n",
    "\n",
    "# Create a smooth line for predictions\n",
    "X_test = np.linspace(-3.5, 3.5, 100).reshape(-1, 1)\n",
    "y_linear = model_linear.predict(X_test)\n",
    "y_quadratic = model_quadratic.predict(X_test)\n",
    "y_complex = model_complex.predict(X_test)\n",
    "\n",
    "# Calculate training error for each model\n",
    "err_linear = np.mean((model_linear.predict(X) - y)**2)\n",
    "err_quadratic = np.mean((model_quadratic.predict(X) - y)**2)\n",
    "err_complex = np.mean((model_complex.predict(X) - y)**2)\n",
    "\n",
    "# Plot the results - 3 separate subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# First plot: Linear model\n",
    "axes[0].scatter(X, y, color='navy', s=30, label='Data points')\n",
    "axes[0].plot(X_test, y_linear, color='red', linewidth=2, label=f'Linear (MSE: {err_linear:.2f})')\n",
    "axes[0].set_title('Linear Model (Underfitting)', fontsize=14)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_ylim(-5, 25)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Second plot: Quadratic model (good fit)\n",
    "axes[1].scatter(X, y, color='navy', s=30, label='Data points')\n",
    "axes[1].plot(X_test, y_quadratic, color='green', linewidth=2, label=f'Quadratic (MSE: {err_quadratic:.2f})')\n",
    "axes[1].set_title('Quadratic Model (Good Fit)', fontsize=14)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].set_ylim(-5, 25)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Third plot: Complex model (perfect overfitting)\n",
    "axes[2].scatter(X, y, color='navy', s=30, label='Data points')\n",
    "axes[2].plot(X_test, y_complex, color='orange', linewidth=2, label=f'Order 30 Polynomial (MSE: {err_complex:.2f})')\n",
    "axes[2].set_title('Order 30 Polynomial (Overfitting)', fontsize=14)\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].set_ylim(-5, 25)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"1. The linear model (order 1) is too simple and underfits the data.\")\n",
    "print(\"2. The quadratic model (order 2) nicely captures the underlying pattern.\")\n",
    "print(\"3. The high-order polynomial (order 30) overfits by catching every data point perfectly.\")\n",
    "print(\"\\nWithout regularization, the complex model tries to pass through every single point,\")\n",
    "print(\"creating wild fluctuations between data points. This is a classic sign of overfitting.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b6770de8a952aa",
   "metadata": {},
   "source": [
    "### Regularization: Teaching Models to Keep It Simple\n",
    "\n",
    "The above example demonstrates the concept of regularization in machine learning. We generated data with a quadratic pattern (y = x¬≤) plus noise and fit models of different complexity:\n",
    "\n",
    "1. **Linear Model (Underfitting)**: Too simple to capture the quadratic nature of the data.\n",
    "2. **Quadratic Model (Good Fit)**: Correctly captures the underlying pattern without overfitting.\n",
    "3. **High-Order Polynomial (Overfitting)**: Creates an unstable function by trying to fit every data point perfectly.\n",
    "4. **Regularized High-Order Polynomial**: Still has high complexity but produces a smoother curve by penalizing extreme coefficient values.\n",
    "\n",
    "**So what exactly is regularization?**\n",
    "\n",
    "Regularization works by adding a penalty term to the loss function that increases with model complexity:\n",
    "\n",
    "$$L_{regularized}(model) = Error(model, data) + \\lambda \\times Complexity(model)$$\n",
    "\n",
    "Where:\n",
    "- $Error(model, data)$ measures how well the model fits the training data\n",
    "- $Complexity(model)$ measures how complex the model is (often using the size of parameters)\n",
    "- $\\lambda$ (lambda) controls the strength of regularization\n",
    "\n",
    "By including this penalty, we tell the model: \"It's not just about fitting the training data perfectly; you should also keep yourself as simple as possible.\"\n",
    "\n",
    "Common regularization techniques include:\n",
    "- **L1 Regularization (Lasso)**: Penalizes the sum of absolute values of weights, often producing sparse models\n",
    "- **L2 Regularization (Ridge)**: Penalizes the sum of squared weights, generally shrinking all weights\n",
    "- **Dropout**: Randomly turns off neurons during training to prevent co-adaptation\n",
    "- **Early Stopping**: Stops training before the model starts overfitting\n",
    "\n",
    "Using regularization is like telling a student: \"It's not just about memorizing the exact answers to homework problems; you need to understand the underlying patterns so you can solve new problems on the test.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1ecd6-577e-4019-9940-062fa25f1f2f",
   "metadata": {},
   "source": [
    "### 4.1.4 A Few words on Preprocessing Before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7836d28-54d5-433d-b2a9-c687983d13be",
   "metadata": {},
   "source": [
    "In real-world cases, we usually preprocess the data further before passing it to a model. **Preprocessing** can involve:  \n",
    "\n",
    "üîπ **Feature Extraction** - Creating additional features from raw data. While this is **less common in images** due to the nature of CNNs, it is **very useful in other models**.\n",
    "* Example (House Prices) ‚Üí Instead of using the raw address, we can preprocess it into ‚Äúdistance from the city center‚Äù, turning an informative but hard-to-use string into a continuous, easy-to-work-with number.\n",
    "* Example (NLP) ‚Üí Before using text in a model, we must convert words into numbers, and better to meaningful numerical vectors (a.k.a. embeddings) to capture their relationships and meanings.\n",
    "  \n",
    "üîπ **Data Manipulation** ‚Äì Standardizing input formats (e.g., resizing images, filtering out low-resolution images, handling missing values).  \n",
    "üîπ **Normalization & Scaling** ‚Äì Ensuring that numerical features are on a similar scale to improve training stability.  \n",
    "\n",
    "##### üöÄ Why Is Preprocessing Crucial?  \n",
    "Preprocessing is usually where a Data Scientist has the most **room to shine**! Unlike modeling, where architectures and optimizers are often well-defined, **there is no single \"correct\" way to preprocess data**.  \n",
    "**_\"It is art\"_** as some Feinshmekers would say  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffc5dc-4232-41e1-a23c-20285be7c0a4",
   "metadata": {},
   "source": [
    "## 4.2 (Really) Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107bd80-e926-4c87-aeb0-19cc8c9c1ef9",
   "metadata": {},
   "source": [
    "**Step 1: Set Up & Choose the Best Device:**\n",
    "\n",
    "\n",
    "Training neural networks is heavy lifting‚Äîso we want to use a GPU if we can. This line lets PyTorch automatically pick the best hardware on your machine."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e32f547-afcb-4e88-b586-94f3ea424801",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Automatically select the best available device:\n",
    "# MPS (Apple Silicon), CUDA (NVIDIA GPU), or CPU fallback\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05b96be2-7d60-48dc-b2da-a4e52ec88dae",
   "metadata": {},
   "source": [
    "**Step 2: Image Preprocessing (Transformations)**\n",
    "\n",
    "Most pretrained models expect inputs to be a specific size and range. This transformation resizes the photo, turns it into numbers, and scales it to the distribution used in ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a81478-71cb-4e28-aed7-242b310165cc",
   "metadata": {},
   "source": [
    "# Define the image transformations to apply\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to MobileNet input size\n",
    "    transforms.ToTensor(),          # Convert to PyTorch tensor\n",
    "    transforms.Normalize(           # Standard ImageNet normalization\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "433d83e4-c4a4-4530-8c8e-b56d7c7f6aad",
   "metadata": {},
   "source": [
    "**‚úÖ Resize:**\n",
    "\n",
    "We resize the image to a fixed shape (e.g., 224x224) because neural networks like MobileNet require all inputs to have the same size. This ensures the architecture can process batches of images and apply convolutional layers without shape mismatches.\n",
    "\n",
    "**‚úÖ Normalize:**\n",
    "\n",
    "Normalization is an optimization technique that adjusts the pixel values to have a standard distribution (usually with mean ‚âà 0 and std ‚âà 1).\n",
    "This helps the training process converge faster and more stably, because it keeps the inputs (and thus activations and gradients) on a consistent scale.\n",
    "It also reduces the risk of numerical instability during backpropagation, especially when layers involve many multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e47a0-02dd-447b-bf4e-d9f6122e604a",
   "metadata": {},
   "source": [
    "**üì¶ Step 3: Load Our Dataset and Create DataLoaders**\n",
    "\n",
    "We use PyTorch‚Äôs ImageFolder to read labeled folders automatically. DataLoader helps us feed images into the model in mini-batches. Note: we shuffle the training set, but not the validation set.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "id": "88af134a-42c1-4495-af0d-fb65eb33bb15",
   "metadata": {},
   "source": [
    "# Paths to your prepared dataset folders\n",
    "train_dir = f\"{OUTPUT_DIR}/train\"\n",
    "val_dir = f\"{OUTPUT_DIR}/val\"\n",
    "test_dir = f\"{OUTPUT_DIR}/test\"\n",
    "\n",
    "# Load image folders with the defined transforms\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Wrap datasets with DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print dataset sizes for sanity check\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8566680-d34a-4b90-adf7-9bebc4786eb3",
   "metadata": {},
   "source": [
    "**üß† Step 4: Load and Adapt a Pretrained Model**\n",
    "\n",
    "We start with a pretrained MobileNetV2‚Äîa lightweight CNN originally trained on ImageNet.\n",
    "Then, we replace its final layer to output just two scores:\n",
    "Like (1) or Dislike (0).\n",
    "This lets the model learn to reflect your photo preferences."
   ]
  },
  {
   "cell_type": "code",
   "id": "849826cf-59ac-41f0-9263-7fe949bfde38",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# draw here the architecture of 224*224 in 2 out\n",
    "from torchsummary import summary\n",
    "\n",
    "# Load pretrained MobileNet model\n",
    "model = models.mobilenet_v2()\n",
    "\n",
    "# Modify the classifier for our binary classification task (Like vs. Dislike)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 2)  # Two output classes (Like & Dislike)\n",
    ")\n",
    "\n",
    "# Temporarily move to CPU for summary\n",
    "model_cpu = model.to(\"cpu\")\n",
    "summary(model_cpu, input_size=(3, 224, 224))\n",
    "\n",
    "# Send it back to the actual device (MPS, CUDA, etc.)\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42b9ebaf-5da4-4076-9cf1-ef7280213bd3",
   "metadata": {},
   "source": [
    "**üìâ Step 5: Assign a Loss Function**\n",
    "\n",
    "Since our goal is to teach the model what‚Äôs ‚Äúwrong‚Äù, we use a loss function.\n",
    "\n",
    "We‚Äôll use Cross-Entropy Loss, ideal for binary classification like Like vs. Dislike:\n",
    "\n",
    "$$\n",
    "L = - \\sum w_i y_i \\log(\\hat{y_i})\n",
    "$$\n",
    "\n",
    "* ‚úÖ Rewards the model for assigning high probability to the correct class\n",
    "* ‚úÖ Penalizes confident wrong guesses more than uncertain ones\n",
    "\n",
    "To handle class imbalance, we assign higher weight to the Like class, so it isn‚Äôt ignored just because it‚Äôs rare.\n",
    "* üîÑ You can adjust this: \n",
    "* High values (15‚Äì20): üìà Prioritize recall (find more potential matches)\n",
    "* Mid values (5‚Äì8): ‚öñÔ∏è Balance precision and recall\n",
    "* 1.0: Treat both classes equally\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2fc94913-e613-4076-8130-db981fd95db6",
   "metadata": {},
   "source": [
    "# Set the weight parameter to balance class importance\n",
    "# Higher value for the \"Like\" class (index 1) means the model will be penalized more for misclassifying \"Likes\"\n",
    "# A good starting point is the inverse of class frequencies\n",
    "dislike_weight = 1.0\n",
    "like_weight = 1.0  # Makes misclassifying \"Likes\" 10x more costly than \"Dislikes\"\n",
    "class_weights = torch.tensor([1.0, like_weight], device=device)\n",
    "\n",
    "# Define loss function with weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96ad7700-677b-46ce-859d-8a7b251664fa",
   "metadata": {},
   "source": [
    "**üõ†Ô∏è Step 6: Train the Model**\n",
    "\n",
    "Time to put everything together and train our model!\n",
    "* We‚Äôll use the Adam optimizer, a modern variant of Gradient Descent\n",
    "* The learning rate (0.001) is a standard starting point in many applications\n",
    "* Each epoch includes:\n",
    "* A training phase ‚Äì update weights on labeled data\n",
    "* A validation phase ‚Äì check performance on unseen examples\n",
    "\n",
    "We‚Äôll also track loss on both sets to visualize learning progress and catch signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "id": "1815ea93-0d04-4c83-ad0c-6ac715ae7acd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import time  # Add this at the top of your notebook/script\n",
    "\n",
    "# Define the optimizer to be Adam (advanced GD) with a learning rate of 0.001 \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 100  # Keep training short for the workshop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # ‚è±Ô∏è Start timing\n",
    "    \n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "print(\"Training complete! üéâ\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5b09fb1a1147723",
   "metadata": {},
   "source": [
    "### Theory Brake! while training our network, let's head to \"What is Neural Network.ipynb\" and learn what is a Neural Network and how does it \"Learn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af0cf193dc3cbe",
   "metadata": {},
   "source": [
    "**üìà Step 7: Visualize the Learning Curve**\n",
    "\n",
    "Let‚Äôs plot the training and validation loss over time.\n",
    "This helps us see if the model is improving, and spot signs of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d669e61-6026-458c-8002-5b27872c88de",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss', marker='o', linestyle='-', markersize=2)\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss', marker='s', linestyle='-', markersize=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(range(1, num_epochs+1, 1))  # Set x-axis ticks to integers from 1 to num_epochs\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92953751-137d-40b7-8328-489001f55542",
   "metadata": {},
   "source": [
    "**‚ùó Uh-oh‚Ä¶ Our Model Is Overfitting!**\n",
    "\n",
    "That might sound bad ‚Äî but it‚Äôs actually expected given our setup:\n",
    "* üßÆ Small dataset ‚Äì Only ~500 images total\n",
    "* ‚öñÔ∏è Imbalanced labels ‚Äì Probably 10 ‚ÄúDislikes‚Äù for every ‚ÄúLike‚Äù\n",
    "* ü§î Problem complexity ‚Äì ‚ÄúTaste‚Äù is slippery and subjective, not easy to define\n",
    "\n",
    "You‚Äôre not expected to fully understand all of these challenges yet.\n",
    "After we cover evaluation, we‚Äôll look at practical ways to improve performance.\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "üîç First and most important fix:\n",
    "**Go over your ‚ÄúLiked‚Äù folder and make sure your labels truly reflect your preferences!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beea8c-1c3d-4f22-bed5-c228f1ac70e0",
   "metadata": {},
   "source": [
    "# 5. Test and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d81245bfbce413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.0 Prediction Types\n",
    "Every prediction falls into one of four boxes. Knowing where your model succeeds or fails helps you decide what to improve.\n",
    "\n",
    "Before diving into metrics, let's understand the four possible outcomes in a (binary) classification:\n",
    "\n",
    "```\n",
    "                 ‚îÇ Predicted \"Like\" ‚îÇ Predicted \"Dislike\"\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Actual \"Like\"    ‚îÇ       TP         ‚îÇ        FN\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Actual \"Dislike\" ‚îÇ       FP         ‚îÇ        TN\n",
    "```\n",
    "\n",
    "üîπ **True Positive (TP)** ‚Äì The model predicted \"Like\" and the actual label is \"Like\".\n",
    "   - Real-world: You find a person attractive, and the model correctly predicts this.\n",
    "   - Real-world: In a spam filter, correctly identifying an actual spam email as spam.\n",
    "\n",
    "üîπ **True Negative (TN)** ‚Äì The model predicted \"Dislike\" and the actual label is \"Dislike\".\n",
    "   - Real-world: You don't find a person attractive, and the model correctly predicts this.\n",
    "   - Real-world: In a medical test, correctly identifying a healthy patient as not having the disease.\n",
    "\n",
    "üîπ **False Positive (FP)** ‚Äì You don‚Äôt like the person, but the model thought you would.\n",
    "   - Real-world: You don't find a person attractive, but the model incorrectly predicts you would.\n",
    "   - Real-world: In fraud detection, flagging a legitimate transaction as fraudulent.\n",
    "   - Also known as a \"Type I error\" in statistics.\n",
    "\n",
    "üîπ **False Negative (FN)** ‚Äì You *do* like the person, but the model didn‚Äôt think so.\n",
    "   - Real-world: You find a person attractive, but the model incorrectly predicts you wouldn't.\n",
    "   - Real-world: In a cancer screening, failing to detect a malignant tumor.\n",
    "   - Also known as a \"Type II error\" in statistics.\n",
    "\n",
    "These four outcomes are the building blocks for all the metrics we'll discuss next, and they can be visualized in a confusion matrix:\n",
    "\n",
    "Think of your model as your enthusiastic dating assistant. Sometimes it gets it right, sometimes it hypes up the wrong person. Understanding these four cases helps you guide it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b4209-8d34-4f5f-a29f-fd0c6b5cbbed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.1 Classification Metrics\n",
    "\n",
    "A metric in general can be any function that measures your performance the way you want. These are some of the most intuitive and used metrics.\n",
    "\n",
    "üîπ **Accuracy** ‚Äì The proportion of all predictions that are correct.  \n",
    "   - Formula: (True Positives + True Negatives) / Total Predictions = (total correct)/ (total predictions)\n",
    "   - Simple but can be misleading with imbalanced classes (e.g., if 90% are \"Dislike\")\n",
    "\n",
    "Of course, we'd love to build models with 100% accuracy, but this is usually impossible in real-world scenarios. Why? Because perfect classification would require:\n",
    "- A perfect prediction might exist. correct output can have inherent randomness (think of example)\n",
    "- Complete information about all factors affecting the outcome\n",
    "- Perfect understanding of the relationship between these factors\n",
    "- Data that captures all possible cases perfectly\n",
    "\n",
    "Since we rarely have all of these, we need to define which types of errors are more acceptable in our specific application. This leads us to more \"focused\" metrics:\n",
    "\n",
    "üîπ **Precision** ‚Äì When the model predicts True (\"Like\"), how often is it correct?  \n",
    "   - Formula: True Positives / (True Positives + False Positives)\n",
    "   - High precision means fewer false positives\n",
    "   - Example: If you want to avoid swiping right on people you don't actually find attractive\n",
    "\n",
    "üîπ **Recall** ‚Äì What proportion of actual Positives (\"Likes\") does the model correctly identify?  \n",
    "   - Formula: True Positives / (True Positives + False Negatives)\n",
    "   - High recall means fewer false negatives\n",
    "   - Example: If you don't want to miss potential matches you might like\n",
    "\n",
    "**The Precision-Recall Tradeoff:**\n",
    "There's an inherent tension between precision and recall. A model that shows you every possible match (high recall) will inevitably include many profiles you don't actually like (low precision). Conversely, a model that's very selective (high precision) might miss many potential matches (low recall).\n",
    "\n",
    "Two extreme cases illustrate this:\n",
    "- A model that predicts \"Like\" for everyone will have 100% recall (you'll see every potential match) but terrible precision (most suggestions won't interest you)\n",
    "- A model that rarely predicts \"Like\" might have high precision (when it suggests someone, it's usually right) but poor recall (it misses many people you would actually like)\n",
    "\n",
    "\n",
    "#### What Does This Tell Us?\n",
    "\n",
    "- **High Precision** but **Low Recall** ‚Äì Model is conservative, showing you good matches, but missing potential matches you might like\n",
    "- **High Recall** but **Low Precision** ‚Äì Model is aggressive, showing many people you actually wouldn't like\n",
    "- **Balanced F1** ‚Äì Good compromise between finding all potential matches and avoiding bad matches\n",
    "\n",
    "#### Real-World Translation\n",
    "\n",
    "Think of these metrics in terms of a dating app:\n",
    "\n",
    "- **Precision** ‚Äì When the app shows you a profile, how likely are you to actually like it?\n",
    "- **Recall** ‚Äì Of all the profiles you would like, what percentage does the app show you?\n",
    "- **F1 Score** ‚Äì Balance between finding all potential matches and not wasting your time with non-matches\n",
    "\n",
    "\n",
    "**How to balance between the two:**\n",
    "\n",
    "1. User perferences. (It usually comes down to this)\n",
    "    - Usually the user (Customer, tinder user...) will have tendancy to which kind of mistakses they prefer\n",
    "    - You might prefer see more options and screen yourself - you probably tend to recall\n",
    "    - You might prefer to not waist your time - you prefet to see 10 out of 20 good options, but each option \"Bonbon\"\n",
    "    - In court - They have tendnancy to Precision (if we convict we are positive they are guilty, and when doube they are not guilty)\n",
    "    - Magnometers - tend to recall. they beep alot and usually it is a FPS . Same for babysense\n",
    "    - Sentra case- tend to precision\n",
    "    - and so on\n",
    "\n",
    "\n",
    "2. **F1 Score** ‚Äì The harmonic mean of precision and recall.\n",
    "   - Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "   - Balances precision and recall, useful when you care about both\n",
    "   - Never used them myself as I believe in preferences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "üîπ **Confusion Matrix** ‚Äì A table showing predicted vs. actual classifications.  \n",
    "   - Helps visualize where the model succeeds and fails\n",
    "   - Shows all four outcomes: True Positives, True Negatives, False Positives, False Negatives\n",
    "\n",
    "Now let's calculate these metrics on our validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efba4a1",
   "metadata": {},
   "source": [
    "## 5.2 Validation - evaulate performance on the validation set\n",
    "\n",
    "The most convient way to understand the performance of the model is via a confusion matrix + summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff8918e3-1926-4256-b39f-df104872f6af",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Lists to store predictions and ground truth\n",
    "all_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients for evaluation\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Move predictions and labels to CPU for sklearn metrics\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_ground_truth.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_ground_truth, all_predictions)\n",
    "precision = precision_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "recall = recall_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "f1 = f1_score(all_ground_truth, all_predictions, zero_division=0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Validation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Create and visualize confusion matrix\n",
    "cm = confusion_matrix(all_ground_truth, all_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Dislike\", \"Like\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b53feed6-2d9e-4da2-b543-864eb570ea3b",
   "metadata": {},
   "source": [
    "### 5.2.1 Understanding the Confusion Matrix\n",
    "\n",
    "The confusion matrix shows us **where our model succeeds and fails**:\n",
    "\n",
    "- **True Positives (Bottom-Right)** ‚Äì Images you like that the model correctly classified as \"Like\"\n",
    "- **True Negatives (Top-Left)** ‚Äì Images you dislike that the model correctly classified as \"Dislike\"\n",
    "- **False Positives (Top-Right)** ‚Äì Images you dislike that the model incorrectly classified as \"Like\"\n",
    "- **False Negatives (Bottom-Left)** ‚Äì Images you like that the model incorrectly classified as \"Dislike\"\n",
    "\n",
    "\n",
    "For most dating apps, they might prioritize **recall** over precision ‚Äì showing you more options rather than missing potential matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cadca70-f1ea-4063-843d-27156742a0da",
   "metadata": {},
   "source": [
    "### 5.2.2 Understanding the Confusion\n",
    "Instead of just looking at the numbers, we can actually examine by eye where our model got wrong. Maybe it \"reasonable\" mistakes ?"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8340c45",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load validation dataset with a transform that doesn't normalize (for display purposes)\n",
    "display_transform = transforms.Compose([\n",
    "  transforms.Resize((224, 224)),\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_paths = []\n",
    "val_labels = []\n",
    "val_predictions = []\n",
    "\n",
    "# Get file paths and model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for root, _, files in os.walk(val_dir):\n",
    "      for file in sorted(files):\n",
    "          if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "              img_path = os.path.join(root, file)\n",
    "              label = 1 if \"Like\" in img_path else 0  # 1 for Like, 0 for Dislike\n",
    "\n",
    "              # Load and transform the image\n",
    "              img = Image.open(img_path).convert('RGB')\n",
    "              img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "              # Get prediction\n",
    "              output = model(img_tensor)\n",
    "              _, prediction = torch.max(output, 1)\n",
    "\n",
    "              val_paths.append(img_path)\n",
    "              val_labels.append(label)\n",
    "              val_predictions.append(prediction.item())\n",
    "\n",
    "# Find images the model predicts as \"Like\" (matches)\n",
    "matches_indices = [i for i, pred in enumerate(val_predictions) if pred == 1]\n",
    "matches_paths = [val_paths[i] for i in matches_indices]\n",
    "matches_true_labels = [val_labels[i] for i in matches_indices]\n",
    "\n",
    "# Find false negatives (you liked but model predicted dislike)\n",
    "false_negatives_indices = [i for i, (label, pred) in enumerate(zip(val_labels, val_predictions))\n",
    "                        if label == 1 and pred == 0]\n",
    "false_negatives_paths = [val_paths[i] for i in false_negatives_indices]\n",
    "\n",
    "# Count correct and incorrect \"Like\" predictions\n",
    "true_positives = sum(1 for i in matches_indices if val_labels[i] == 1)\n",
    "false_positives = sum(1 for i in matches_indices if val_labels[i] == 0)\n",
    "\n",
    "# Display matches first (True predictions)\n",
    "num_matches_to_display = min(8, len(matches_paths))\n",
    "if num_matches_to_display > 0:\n",
    "  fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for i in range(num_matches_to_display):\n",
    "      img = Image.open(matches_paths[i]).convert('RGB')\n",
    "      img = img.resize((224, 224))  # Resize for display\n",
    "      actual_label = \"Actually Like ‚úì\" if matches_true_labels[i] == 1 else \"Actually Dislike ‚úó\"\n",
    "\n",
    "      axes[i].imshow(np.array(img))\n",
    "      axes[i].set_title(f\"{actual_label}\", fontsize=10)\n",
    "      axes[i].axis('off')\n",
    "\n",
    "  # Hide any unused subplots\n",
    "  for i in range(num_matches_to_display, len(axes)):\n",
    "      axes[i].axis('off')\n",
    "\n",
    "  fig.suptitle(\"Model Predictions: Profiles It Would Show You\", fontsize=16)\n",
    "\n",
    "  # Print summary statistics\n",
    "  print(f\"Out of {len(val_paths)} profiles in the validation set:\")\n",
    "  print(f\"- Your model would show you {len(matches_paths)} profiles ({len(matches_paths)/len(val_paths)*100:.1f}%)\")\n",
    "  print(f\"- {true_positives} of these you actually liked ({true_positives/len(matches_paths)*100:.1f}% precision)\")\n",
    "  print(f\"- The model found {true_positives} out of {sum(val_labels)} profiles you actually liked ({true_positives/sum(val_labels)*100:.1f}% recall)\")\n",
    "else:\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  plt.text(0.5, 0.5, \"No matches found!\", ha='center', va='center', fontsize=14)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "# Now display False Negatives (profiles you liked but model missed)\n",
    "num_false_neg_to_display = min(5, len(false_negatives_paths))\n",
    "if num_false_neg_to_display > 0:\n",
    "  fig, axes = plt.subplots(1, num_false_neg_to_display, figsize=(4*num_false_neg_to_display, 4))\n",
    "\n",
    "  # Handle the case where we only have one false negative\n",
    "  if num_false_neg_to_display == 1:\n",
    "      axes = [axes]\n",
    "\n",
    "  for i in range(num_false_neg_to_display):\n",
    "      img = Image.open(false_negatives_paths[i]).convert('RGB')\n",
    "      img = img.resize((224, 224))  # Resize for display\n",
    "\n",
    "      axes[i].imshow(np.array(img))\n",
    "      axes[i].set_title(f\"Missed Match\", fontsize=10)\n",
    "      axes[i].axis('off')\n",
    "\n",
    "  fig.suptitle(\"False Negatives: Profiles You Liked But Model Missed\", fontsize=16)\n",
    "  print(f\"\\nFalse Negatives:\")\n",
    "  print(f\"- The model missed {len(false_negatives_paths)} profiles you actually liked\")\n",
    "  print(f\"- These represent {len(false_negatives_paths)/sum(val_labels)*100:.1f}% of your total 'Like' preferences\")\n",
    "else:\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  plt.text(0.5, 0.5, \"No false negatives found!\", ha='center', va='center', fontsize=14)\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1bc75db",
   "metadata": {},
   "source": [
    "## 5.4.1 Possible fixes - Analyzing Model Confidence\n",
    "In order to direct a model to better performance we should panelize/reward towards the desired outcome. this can be implemented in several ways:\n",
    "1. Panelize the model more on specific type of mistakes *In training time*\n",
    "2. Lower the tresholds in which the model decide on like. By default, the NN models assign the label that gets > 0.5 score. We can lower/higher the treshold according to our needs.\n",
    "   \n",
    "Let's examine how confident the model is about each \"Like\" prediction. This can help us understand which features most strongly influence the model's decisions:"
   ]
  },
  {
   "cell_type": "code",
   "id": "de93b64c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Process all validation images and get confidence scores\n",
    "val_images = []\n",
    "val_confidences = []\n",
    "val_predictions = []\n",
    "val_actual_labels = []\n",
    "val_paths = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for root, _, files in os.walk(val_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"Like\" in img_path else 0  # 1 for Like, 0 for Dislike\n",
    "                \n",
    "                # Load and transform the image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get prediction with confidence\n",
    "                output = model(img_tensor)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                confidence = probabilities[0][1].item()  # Confidence for \"Like\" class\n",
    "                prediction = 1 if confidence > 0.5 else 0\n",
    "                \n",
    "                # Store all predictions\n",
    "                val_images.append(img)\n",
    "                val_confidences.append(confidence)\n",
    "                val_predictions.append(prediction)\n",
    "                val_actual_labels.append(label)\n",
    "                val_paths.append(img_path)\n",
    "\n",
    "# 1. First, handle \"Like\" predictions (same as original code)\n",
    "like_indices = [i for i, pred in enumerate(val_predictions) if pred == 1]\n",
    "like_images = [val_images[i] for i in like_indices]\n",
    "like_confidences = [val_confidences[i] for i in like_indices]\n",
    "like_labels = [val_actual_labels[i] for i in like_indices]\n",
    "\n",
    "# Sort \"Like\" predictions by confidence\n",
    "like_sorted_indices = np.argsort(like_confidences)[::-1]\n",
    "like_sorted_images = [like_images[i] for i in like_sorted_indices]\n",
    "like_sorted_confidences = [like_confidences[i] for i in like_sorted_indices]\n",
    "like_sorted_labels = [like_labels[i] for i in like_sorted_indices]\n",
    "\n",
    "# Display \"Like\" predictions\n",
    "num_likes = len(like_sorted_images)\n",
    "if num_likes > 0:\n",
    "    # Set up figure for like predictions\n",
    "    fig, axes = plt.subplots(num_likes, 2, figsize=(12, 2.5 * num_likes), \n",
    "                            gridspec_kw={'width_ratios': [1, 3]})\n",
    "    \n",
    "    # If only one image, axes won't be 2D, so make it 2D\n",
    "    if num_likes == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    # Plot each image and its confidence bar\n",
    "    for i in range(num_likes):\n",
    "        # Display the image\n",
    "        img = like_sorted_images[i].resize((224, 224))\n",
    "        axes[i, 0].imshow(np.array(img))\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display confidence bar\n",
    "        confidence = like_sorted_confidences[i]\n",
    "        bar_colors = ['green' if like_sorted_labels[i] == 1 else 'red']\n",
    "        actual_label = \"Actually Like ‚úì\" if like_sorted_labels[i] == 1 else \"Actually Dislike ‚úó\"\n",
    "        \n",
    "        axes[i, 1].barh([0], [confidence], color=bar_colors, height=0.5)\n",
    "        axes[i, 1].barh([0], [1-confidence], left=[confidence], color='lightgray', height=0.5, alpha=0.5)\n",
    "        axes[i, 1].set_xlim(0, 1.0)\n",
    "        axes[i, 1].set_yticklabels([])\n",
    "        axes[i, 1].set_yticks([])\n",
    "        axes[i, 1].set_xticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axes[i, 1].set_title(f\"Confidence: {confidence:.2f} - {actual_label}\", loc='left')\n",
    "        axes[i, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Model Confidence for 'Like' Predictions (Ordered by Confidence)\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis of confidence for \"Like\" predictions\n",
    "    true_positives = sum(1 for i in range(num_likes) if like_sorted_labels[i] == 1)\n",
    "    false_positives = num_likes - true_positives\n",
    "    \n",
    "    print(f\"Analysis of model confidence for 'Like' predictions:\")\n",
    "    print(f\"- Average confidence for true positives: {sum(like_sorted_confidences[i] for i in range(num_likes) if like_sorted_labels[i] == 1) / max(1, true_positives):.2f}\")\n",
    "    print(f\"- Average confidence for false positives: {sum(like_sorted_confidences[i] for i in range(num_likes) if like_sorted_labels[i] == 0) / max(1, false_positives):.2f}\")\n",
    "    \n",
    "    if true_positives > 0 and false_positives > 0:\n",
    "        print(\"\\nObservation:\")\n",
    "        # Calculate averages separately to avoid line break issues\n",
    "        tp_avg = sum(like_sorted_confidences[i] for i in range(num_likes) if like_sorted_labels[i] == 1) / true_positives\n",
    "        fp_avg = sum(like_sorted_confidences[i] for i in range(num_likes) if like_sorted_labels[i] == 0) / false_positives\n",
    "        \n",
    "        if tp_avg > fp_avg:\n",
    "            print(\"The model is more confident about its correct predictions than its incorrect ones - a good sign!\")\n",
    "        else:\n",
    "            print(\"The model is more confident about some incorrect predictions than correct ones - this suggests room for improvement.\")\n",
    "else:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.text(0.5, 0.5, \"No 'Like' predictions found\", ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Now, handle false negatives (missed matches)\n",
    "false_negative_indices = [i for i, (pred, label) in enumerate(zip(val_predictions, val_actual_labels)) \n",
    "                         if pred == 0 and label == 1]\n",
    "false_neg_images = [val_images[i] for i in false_negative_indices]\n",
    "false_neg_confidences = [val_confidences[i] for i in false_negative_indices]\n",
    "false_neg_paths = [val_paths[i] for i in false_negative_indices]\n",
    "\n",
    "# Sort false negatives by confidence (highest first)\n",
    "false_neg_sorted_indices = np.argsort(false_neg_confidences)[::-1]\n",
    "false_neg_sorted_images = [false_neg_images[i] for i in false_neg_sorted_indices]\n",
    "false_neg_sorted_confidences = [false_neg_confidences[i] for i in false_neg_sorted_indices]\n",
    "\n",
    "# Display false negatives\n",
    "num_false_negs = len(false_neg_sorted_images)\n",
    "if num_false_negs > 0:\n",
    "    # Set up figure for false negatives\n",
    "    fig, axes = plt.subplots(min(num_false_negs, 5), 2, figsize=(12, 2.5 * min(num_false_negs, 5)), \n",
    "                            gridspec_kw={'width_ratios': [1, 3]})\n",
    "    \n",
    "    # If only one image, axes won't be 2D, so make it 2D\n",
    "    if min(num_false_negs, 5) == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    # Plot each image and its confidence bar (up to 5)\n",
    "    for i in range(min(num_false_negs, 5)):\n",
    "        # Display the image\n",
    "        img = false_neg_sorted_images[i].resize((224, 224))\n",
    "        axes[i, 0].imshow(np.array(img))\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display confidence bar\n",
    "        confidence = false_neg_sorted_confidences[i]\n",
    "        \n",
    "        axes[i, 1].barh([0], [confidence], color='orange', height=0.5)\n",
    "        axes[i, 1].barh([0], [1-confidence], left=[confidence], color='lightgray', height=0.5, alpha=0.5)\n",
    "        axes[i, 1].set_xlim(0, 1.0)\n",
    "        axes[i, 1].set_yticklabels([])\n",
    "        axes[i, 1].set_yticks([])\n",
    "        axes[i, 1].set_xticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axes[i, 1].set_title(f\"Confidence: {confidence:.2f} - Missed Match\", loc='left')\n",
    "        axes[i, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Missed Matches: Profiles You Liked But Model Rejected\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Make room for suptitle\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis of confidence for false negatives\n",
    "    print(f\"\\nAnalysis of missed matches (false negatives):\")\n",
    "    print(f\"- Total missed matches: {num_false_negs}\")\n",
    "    print(f\"- Average confidence for missed matches: {sum(false_neg_sorted_confidences) / num_false_negs:.2f}\")\n",
    "    \n",
    "    # Find how close the false negatives were to the threshold\n",
    "    near_threshold = sum(1 for conf in false_neg_sorted_confidences if conf >= 0.4)\n",
    "    print(f\"- Missed matches with confidence above 0.4: {near_threshold} ({near_threshold/num_false_negs*100:.1f}%)\")\n",
    "    \n",
    "    # Insights about threshold adjustment\n",
    "    if near_threshold > 0:\n",
    "        print(\"\\nInsight: Lowering your confidence threshold from 0.5 to 0.4 would capture\")\n",
    "        print(f\"{near_threshold} additional profiles you liked, but might also increase false positives.\")\n",
    "else:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.text(0.5, 0.5, \"No missed matches found!\", ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09003e49-0f01-4b2a-8bff-b510636d5035",
   "metadata": {},
   "source": [
    "# Interactive Threshold Adjustment\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "# Process all validation images with their confidence scores (reuse from previous cell)\n",
    "all_confidences = val_confidences\n",
    "all_labels = val_actual_labels\n",
    "all_images = val_images  # These should be the PIL images\n",
    "\n",
    "# Define function for adjusting threshold\n",
    "def adjust_threshold(threshold=0.5):\n",
    "    # Apply the threshold to make predictions\n",
    "    adjusted_predictions = [1 if conf >= threshold else 0 for conf in all_confidences]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    true_positives = sum(1 for pred, label in zip(adjusted_predictions, all_labels) if pred == 1 and label == 1)\n",
    "    false_positives = sum(1 for pred, label in zip(adjusted_predictions, all_labels) if pred == 1 and label == 0)\n",
    "    false_negatives = sum(1 for pred, label in zip(adjusted_predictions, all_labels) if pred == 0 and label == 1)\n",
    "    true_negatives = sum(1 for pred, label in zip(adjusted_predictions, all_labels) if pred == 0 and label == 0)\n",
    "    \n",
    "    # Count total predictions for \"Like\"\n",
    "    total_like_predictions = sum(1 for pred in adjusted_predictions if pred == 1)\n",
    "    \n",
    "    # Calculate metrics (with safety for division by zero)\n",
    "    precision = true_positives / max(1, total_like_predictions)\n",
    "    recall = true_positives / max(1, true_positives + false_negatives)\n",
    "    accuracy = (true_positives + true_negatives) / len(all_labels)\n",
    "    f1 = 2 * (precision * recall) / max(0.001, precision + recall)  # Avoid division by zero\n",
    "    \n",
    "    # Create the plot for metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Plot 1: Confusion matrix as a heatmap\n",
    "    cm = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n",
    "    im = ax1.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax1.set_title(f'Confusion Matrix (Threshold = {threshold:.2f})')\n",
    "    ax1.set_xticks([0, 1])\n",
    "    ax1.set_yticks([0, 1])\n",
    "    ax1.set_xticklabels(['Predicted Dislike', 'Predicted Like'])\n",
    "    ax1.set_yticklabels(['Actual Dislike', 'Actual Like'])\n",
    "    \n",
    "    # Add text annotations to the confusion matrix\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax1.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    # Plot 2: Metrics bar chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [accuracy, precision, recall, f1]\n",
    "    bars = ax2.bar(metrics, values)\n",
    "    \n",
    "    # Color the bars based on values\n",
    "    for bar, value in zip(bars, values):\n",
    "        if value < 0.33:\n",
    "            bar.set_color('red')\n",
    "        elif value < 0.66:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    \n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.set_title(f'Performance Metrics (Threshold = {threshold:.2f})')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add text labels for the exact values\n",
    "    for i, v in enumerate(values):\n",
    "        ax2.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    # Display statistics as text\n",
    "    plt.figtext(0.5, 0.01, \n",
    "                f\"At threshold {threshold:.2f}:\\n\"\n",
    "                f\"‚Ä¢ Would show you {total_like_predictions} out of {len(all_labels)} profiles ({total_like_predictions/len(all_labels)*100:.1f}%)\\n\"\n",
    "                f\"‚Ä¢ {true_positives} of these you actually liked ({precision*100:.1f}% precision)\\n\"\n",
    "                f\"‚Ä¢ The model found {true_positives} out of {true_positives + false_negatives} profiles you liked ({recall*100:.1f}% recall)\",\n",
    "                ha=\"center\", fontsize=12, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # Now display the profiles with the current threshold\n",
    "    # 1. Get indices of profiles that would be shown (predicted as \"Like\")\n",
    "    matched_indices = [i for i, pred in enumerate(adjusted_predictions) if pred == 1]\n",
    "    \n",
    "    if len(matched_indices) > 0:\n",
    "        # Calculate grid size\n",
    "        max_images = min(8, len(matched_indices))  # Cap at 8 images\n",
    "        cols = min(4, max_images)\n",
    "        rows = math.ceil(max_images / cols)\n",
    "        \n",
    "        # Create the figure for showing matches\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "        \n",
    "        # Make sure axes is 2D even if there's only one row or column\n",
    "        if rows == 1 and cols == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif rows == 1:\n",
    "            axes = np.array([axes])\n",
    "        elif cols == 1:\n",
    "            axes = np.array([[ax] for ax in axes])\n",
    "        \n",
    "        # Flatten the axes for easier indexing\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Display each matched profile\n",
    "        for i in range(max_images):\n",
    "            idx = matched_indices[i]\n",
    "            img = all_images[idx].resize((224, 224))\n",
    "            actual_label = \"Actually Like ‚úì\" if all_labels[idx] == 1 else \"Actually Dislike ‚úó\"\n",
    "            confidence_val = all_confidences[idx]\n",
    "            \n",
    "            axes[i].imshow(np.array(img))\n",
    "            axes[i].set_title(f\"{confidence_val:.2f} - {actual_label}\", fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for i in range(max_images, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f\"Profiles Your Model Would Show (Threshold = {threshold:.2f})\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)  # Make room for suptitle\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.text(0.5, 0.5, f\"No profiles would be shown at threshold {threshold:.2f}!\", \n",
    "                 ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Create interactive widget with slider\n",
    "interact(adjust_threshold, \n",
    "         threshold=FloatSlider(min=0.0, max=1.0, step=0.05, value=0.5, \n",
    "                             description='Threshold:',\n",
    "                             style={'description_width': 'initial'},\n",
    "                             layout={'width': '500px'}));"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebde1c00-4024-410a-bbd3-46999b6df396",
   "metadata": {},
   "source": [
    "## 5.3 Final Evaluation on Test Set\n",
    "\n",
    "Now it's time for the final evaluation on our **test set** ‚Äì data the model has never seen before. This gives us the most realistic measure of how well our model will perform in the real world."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2045b91-5f68-4493-8647-f3bee1811776",
   "metadata": {},
   "source": [
    "## Set the threshold according to your validation and precision-recall preferences\n",
    "threshold = 0.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "335a3ffc-4cee-40ab-9ea0-bcd4be6685c2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load test dataset with a transform that doesn't normalize (for display purposes)\n",
    "display_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_paths = []\n",
    "test_labels = []\n",
    "actual_predictions = []\n",
    "\n",
    "# Get file paths and model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for root, _, files in os.walk(test_dir):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"Like\" in img_path else 0  # 1 for Like, 0 for Dislike\n",
    "                \n",
    "                # Load and transform the image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get prediction\n",
    "                output = model(img_tensor)\n",
    "                # Apply softmax to get probabilities\n",
    "                probs = torch.softmax(output, dim=1)\n",
    "\n",
    "                # Apply threshold manually on class 1\n",
    "                prediction = (probs[0, 1] > threshold).int()\n",
    "                # _, prediction = torch.max(output, 1)\n",
    "                \n",
    "                test_paths.append(img_path)\n",
    "                test_labels.append(label)\n",
    "                actual_predictions.append(prediction.item())\n",
    "\n",
    "# Find images the model predicts as \"Like\"\n",
    "matches_indices = [i for i, pred in enumerate(actual_predictions) if pred == 1]\n",
    "matches_paths = [test_paths[i] for i in matches_indices]\n",
    "matches_true_labels = [test_labels[i] for i in matches_indices]\n",
    "\n",
    "# Count correct and incorrect \"Like\" predictions\n",
    "true_positives = sum(1 for i in matches_indices if test_labels[i] == 1)\n",
    "false_positives = sum(1 for i in matches_indices if test_labels[i] == 0)\n",
    "\n",
    "# Display some of the matches (up to 8)\n",
    "num_to_display = min(8, len(matches_paths))\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8)) if num_to_display > 0 else plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "if num_to_display > 0:\n",
    "    axes = axes.flatten()\n",
    "    for i in range(num_to_display):\n",
    "        img = Image.open(matches_paths[i]).convert('RGB')\n",
    "        img = img.resize((224, 224))  # Resize for display\n",
    "        actual_label = \"Actually Like ‚úì\" if matches_true_labels[i] == 1 else \"Actually Dislike ‚úó\"\n",
    "        \n",
    "        axes[i].imshow(np.array(img))\n",
    "        axes[i].set_title(f\"{actual_label}\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_to_display, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "    fig.suptitle(\"Profiles Your Model Would Show You\", fontsize=16)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Out of {len(test_paths)} profiles in the test set:\")\n",
    "    print(f\"- Your model would show you {len(matches_paths)} profiles ({len(matches_paths)/len(test_paths)*100:.1f}%)\")\n",
    "    print(f\"- {true_positives} of these you actually liked ({true_positives/len(matches_paths)*100:.1f}% precision)\")\n",
    "    print(f\"- The model found {true_positives} out of {sum(test_labels)} profiles you actually liked ({true_positives/sum(test_labels)*100:.1f}% recall)\")\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No matches found!\", ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f0a933c",
   "metadata": {},
   "source": [
    "## 5.5 Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully built a personalized photo preference model. This model actually learns what *you* find attractive, rather than following some pre-defined standard.\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. Overviewed ML and NN theory.\n",
    "2. Created a labeled dataset based on your preferences\n",
    "3. Preprocessed the data for our model\n",
    "4. Trained a convolutional neural network to recognize your preferences\n",
    "5. Evaluated the model's performance with multiple metrics - precision vs recall.\n",
    "6. Played with hyperparameters - loss weights, threshold\n",
    "7. Simulated how this model would work in a real-world application\n",
    "\n",
    "### üöÄ Taking It Further:\n",
    "\n",
    "If you want to keep building from here, there are **four great directions** you can explore:\n",
    "\n",
    "---\n",
    "\n",
    "#### üé• 1. Better Intuition on Nueral Netwoks\n",
    "Get a visual feel for what neural networks do and how they learn.\n",
    "\n",
    "- [**What is a neural network?** ‚Äì 3Blue1Brown (YouTube)](https://www.youtube.com/watch?v=aircAruvnKk)  \n",
    "  One of the best short, intuitive explanations out there.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìö 2. Theory Deep Dive\n",
    "Deepen your understanding of the math, ideas, and guarantees behind ML.\n",
    "\n",
    "**Videos & Courses:**\n",
    "- [**Learning From Data (Caltech)** ‚Äì Full video course + slides](https://work.caltech.edu/telecourse.html)  \n",
    "- [**Andrew Ng‚Äôs Coursera courses**](https://www.coursera.org/specializations/machine-learning-introduction)\n",
    "\n",
    "**Books:**\n",
    "- [*Understanding Machine Learning* ‚Äì Shai Shalev-Shwartz & Shai Ben-David](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)\n",
    "- [*Foundations of Machine Learning* ‚Äì Mohri, Rostamizadeh & Talwalkar](https://cs.nyu.edu/~mohri/mlbook/)\n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ 3. Get Practical  \n",
    "Try working with structured datasets where evaluation is clearer.\n",
    "- Play with this repo What is neural network notebook\n",
    "\n",
    "Here are two **beginner-friendly Kaggle challenges**:\n",
    "\n",
    "- [**Titanic ‚Äì Machine Learning from Disaster**](https://www.kaggle.com/competitions/titanic)  \n",
    "  Predict survival on the Titanic ‚Äî great for learning preprocessing, training, and evaluation.\n",
    "\n",
    "- [**Dogs vs. Cats**](https://www.kaggle.com/competitions/dogs-vs-cats)  \n",
    "  Classic binary image classification using CNNs. Our current session will be a very solid start for this problem.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ 4. Refine *This* Project  \n",
    "\n",
    "If you wanted to improve this model, you could:\n",
    "\n",
    "1. **Collect More Data**: The more examples your model sees, the better it can learn your preferences\n",
    "2. **Fine-Tune Hyperparameters**: Adjust learning rate, batch size, loss weithgs, different splits and even other model architecture\n",
    "3. **Use Data Augmentation**: Create variations of your training images to improve generalization\n",
    "4. **Apply Regularization**: Add techniques like dropout to prevent overfitting\n",
    "\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "The techniques you've learned here extend far beyond dating preferences:\n",
    "- Content recommendation systems\n",
    "- Medical image classification\n",
    "- Quality control in manufacturing\n",
    "- Security applications\n",
    "- And many more!\n",
    "\n",
    "The core principles remain the same: collect quality data, choose an appropriate model, train carefully, and evaluate thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341fb1d-c3ff-4039-9160-db345726aae1",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998585e-bdc1-4300-aec7-ea1d38270e74",
   "metadata": {},
   "source": [
    "## What Are We Actually Optimizing?\n",
    "\n",
    "When we talk about gradient descent, it's important to understand **what function** we're trying to minimize. Let's clarify:\n",
    "\n",
    "### The Loss Function: Measuring Prediction Error\n",
    "\n",
    "In supervised learning, our goal is to make our model's predictions match the true labels:\n",
    "\n",
    "$$\\text{Loss} = \\text{Error between prediction and truth} = L(f(x), y)$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is our model's prediction for input $x$\n",
    "- $y$ is the ground truth label\n",
    "- $L$ is a function that measures how wrong our prediction is\n",
    "\n",
    "For example, in binary classification (like our Tinder model), we use cross-entropy loss:\n",
    "\n",
    "$$L = -y \\log(f(x)) - (1-y) \\log(1-f(x))$$\n",
    "\n",
    "### Why We Only Tune f(x)\n",
    "\n",
    "An important insight: **we can only adjust our model's predictions** $f(x)$, not the ground truth $y$. This is because:\n",
    "\n",
    "1. The ground truth $y$ is fixed - these are the actual labels in our dataset\n",
    "2. Our model $f(x)$ is what we're trying to learn - it contains all the parameters (weights and biases)\n",
    "3. We want $f(x)$ to get as close as possible to $y$ for all examples in our dataset\n",
    "\n",
    "### How Gradient Descent Fits In\n",
    "\n",
    "If we simplify our loss to $L(x) = f(x) - y$, something important becomes clear. When we calculate the gradient:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial (f(x) - y)}{\\partial \\theta} = \\frac{\\partial f(x)}{\\partial \\theta} - \\frac{\\partial y}{\\partial \\theta}$$\n",
    "\n",
    "Since $y$ is our ground truth label and doesn't depend on the model parameters $\\theta$, we have:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial \\theta} = 0$$\n",
    "\n",
    "So the gradient simplifies to:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial f(x)}{\\partial \\theta}$$\n",
    "\n",
    "This means **gradient descent only affects $f(x)$** - our model's predictions - as it should!\n",
    "\n",
    "For each parameter $\\theta$ in our model:\n",
    "1. Calculate how changing $\\theta$ affects the model prediction: $\\frac{\\partial f(x)}{\\partial \\theta}$\n",
    "2. Move $\\theta$ in the direction that makes the prediction closer to $y$: $\\theta := \\theta - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}$\n",
    "\n",
    "The beauty of this approach is that **we don't need to know the exact formula for $f$** in advance. We just need a model flexible enough to learn the pattern, and gradient descent finds the parameters that make the model work."
   ]
  },
  {
   "cell_type": "code",
   "id": "25342855-2e0a-4d7f-8dec-c37cc85673d5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML Workshop)",
   "language": "python",
   "name": "mlworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
